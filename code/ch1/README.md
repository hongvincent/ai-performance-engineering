# Chapter 1: Performance Basics

## Overview

This chapter establishes the foundation for all performance optimization work. Learn to profile code, identify bottlenecks, and apply fundamental optimizations that deliver 5-10x speedups. These techniques are universally applicable and form the basis for more advanced optimizations in later chapters.

## Learning Objectives

After completing this chapter, you can:

- [OK] Profile Python/PyTorch code to identify performance bottlenecks
- [OK] Measure goodput (useful compute vs total time) to quantify efficiency
- [OK] Apply memory management optimizations (pinned memory, preallocated buffers)
- [OK] Use batched operations to improve GPU utilization
- [OK] Leverage CUDA Graphs to reduce kernel launch overhead
- [OK] Understand when and how to apply fundamental optimizations

## Prerequisites

**None** - This is the foundation chapter. Start here!

**Hardware**: NVIDIA GPU

**Software**: PyTorch 2.x+, CUDA 12.x+

## Examples

### 1. `performance_basics.py` - Baseline Implementation

**Purpose**: Establish baseline goodput measurement methodology.

**What it demonstrates**:
- Measuring goodput (useful compute time / total iteration time)
- Basic training loop structure
- Common inefficiencies in naive implementations

**How to run**:
```bash
python3 performance_basics.py
```

**Expected output**:
```
goodput=X% (useful=Xs total=Xs)
```

Typical baseline: **40-60% goodput** (significant overhead!)

---

### 2. `performance_basics_optimized.py` - Optimized Implementation

**Purpose**: Apply fundamental optimizations and measure cumulative impact.

**Optimizations demonstrated**:

#### Optimization 1: Preallocated Tensors
**Problem**: `torch.randn()` and `torch.randint()` create new tensors on each iteration, causing 210ms CPU overhead (seen in profiling).

**Solution**: Preallocate device buffers once, reuse across iterations:
```python
data_buf = torch.empty(batch_size, 256, device=device)
target_buf = torch.empty(batch_size, dtype=torch.long, device=device)
```

**Impact**: Eliminates 210ms overhead per batch → ~2x speedup

#### Optimization 2: Pinned Memory DataLoader
**Problem**: Unpinned memory requires CPU staging buffer for H2D transfers.

**Solution**: Enable `pin_memory=True` in DataLoader:
```python
DataLoader(dataset, batch_size=32, pin_memory=True, num_workers=4)
```

**Impact**: 2-6x faster H2D transfers (varies by system)

#### Optimization 3: CUDA Graphs
**Problem**: Each kernel launch has ~5-20μs overhead. Small kernels spend more time launching than computing!

**Solution**: Capture static computation graph:
```python
graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph):
    output = model(input)
graph.replay()  # Much faster than re-launching
```

**Impact**: ~50-70% reduction in launch overhead

#### Optimization 4: Larger Batch Sizes
**Problem**: Small batches (32) underutilize GPU (low MFLOPs).

**Solution**: Increase batch size to saturate compute:
```python
batch_size = 128  # or 256, 512 depending on model/memory
```

**Impact**: 87 MFLOPs → 1000+ MFLOPs (10x+ GEMM efficiency)

**How to run**:
```bash
python3 performance_basics_optimized.py
```

**Expected output**:
```
Performance Optimization Comparison
====================================

Test 1: Original Implementation
Original - goodput=X% (useful=Xs total=Xs)

Test 2: Pinned Memory DataLoader
Pinned Memory - goodput=Y% (useful=Ys total=Ys)

Test 3: CUDA Graphs + Larger Batch
Optimized (CUDA Graphs) - goodput=Z% (useful=Zs total=Zs)
Speedup vs baseline: ~Ax

Test 4: Batch Size Impact
Batch  32: X.XX ms/iter, XXX MFLOPs
Batch  64: X.XX ms/iter, XXX MFLOPs
Batch 128: X.XX ms/iter, XXXX MFLOPs
Batch 256: X.XX ms/iter, XXXX MFLOPs
```

**Expected overall speedup**: **2-5x** (varies by workload)

---

### 3. `batched_gemm_example.cu` - CUDA Batched GEMM

**Purpose**: Demonstrate importance of batched operations at CUDA level.

**Problem observed in profiling**: Training loop launched 40 separate GEMMs sequentially:
- Each launch: ~10μs overhead
- Total overhead: 400μs per batch
- Poor kernel fusion opportunities

**Solution**: Use cuBLAS batched GEMM API:
```cpp
cublasSgemmBatched(handle, ..., batch_count);
```

**How to run**:
```bash
cd ch1
make
./batched_gemm_example
```

**Expected output**:
```
Individual GEMMs: XXX ms
Batched GEMM:     YYY ms
Speedup:          31.2x
```

**Typical speedup**: **20-40x** (more dramatic for small matrices)

**Key insight**: This is why PyTorch automatically batches operations internally!

---

### 4. `roofline_analysis.py` - Roofline Performance Model

**Purpose**: Implement roofline analysis to classify kernels as compute-bound or memory-bound.

**What it demonstrates**:
- Calculating arithmetic intensity (FLOP/Byte) for different operations
- Plotting kernels on the roofline model for NVIDIA GPUs
- Identifying whether optimizations should target compute or memory bandwidth
- Comparing vector operations (memory-bound) vs matrix operations (compute-bound)

**Key concepts**:
- **Roofline model**: Performance ceiling defined by either compute peak or memory bandwidth
- **Ridge point**: Arithmetic intensity where compute and bandwidth ceilings intersect
- **Example specs**: Modern NVIDIA GPUs achieve high TFLOPS and memory bandwidth
- **Optimization strategy**: Memory-bound kernels need better data reuse; compute-bound kernels need better instruction mix

**How to run**:
```bash
python3 roofline_analysis.py
```

**Expected output**:
```
Vector Add:
  AI: 0.0833 FLOP/Byte
  Achieved: 0.45 TFLOPS
  Memory-bound (AI << 250)

Matrix Multiply:
  AI: 682.67 FLOP/Byte
  Achieved: 145.23 TFLOPS
  Compute-bound (AI > 250)

Roofline plot saved to roofline_plot.png
```

---

### Chapter profiling

Chapter profiling is handled by `ch1/compare.py`. Run it from the project root:

```bash
python3 -c "from ch1.compare import profile; profile()"
```

Or run benchmarks using the unified entry point:
```bash
python benchmark.py --chapter 1
```

**Key insight**: Operations below the ridge point are limited by memory bandwidth, not compute!

---

### 5. `arithmetic_intensity_demo_sm100` - Kernel Optimization Strategies

**Purpose**: Show five kernel optimization techniques and their impact on arithmetic intensity.

**What it demonstrates**:
- **Baseline**: Simple element-wise kernel
- **Loop unrolling**: Reduce branch overhead, expose ILP
- **Vectorized loads**: Use `float4` to load 128 bits at once
- **Increased FLOPs**: Add useful work to improve AI
- **Kernel fusion**: Combine multiple passes to eliminate memory traffic

**Performance progression**:
```
Baseline:     125 GB/s,  AI = 0.25 FLOP/Byte
Unrolled:     145 GB/s,  AI = 0.25 FLOP/Byte (better utilization)
Vectorized:   245 GB/s,  AI = 0.25 FLOP/Byte (coalescing + bandwidth)
More FLOPs:   280 GB/s,  AI = 1.50 FLOP/Byte (6x better AI!)
Fused:        420 GB/s,  AI = 3.00 FLOP/Byte (12x better AI!)
```

**How to run**:
```bash
make arithmetic_intensity_demo
./arithmetic_intensity_demo_sm100
```

**Expected output**:
```
Arithmetic Intensity Optimization Demo (N = 10M elements)

Baseline kernel:
  Time: 2.45 ms, Bandwidth: 122.4 GB/s, AI: 0.25 FLOP/Byte

Unrolled kernel:  
  Time: 2.12 ms, Bandwidth: 141.5 GB/s, AI: 0.25 FLOP/Byte

Vectorized kernel:
  Time: 1.24 ms, Bandwidth: 241.9 GB/s, AI: 0.25 FLOP/Byte

Optimized kernel (more FLOPs):
  Time: 1.08 ms, Bandwidth: 277.8 GB/s, AI: 1.50 FLOP/Byte

Fused kernel:
  Time: 0.72 ms, Bandwidth: 416.7 GB/s, AI: 3.00 FLOP/Byte
  Overall speedup: 3.4x
```

**Key insight**: Increasing arithmetic intensity (more FLOPs per byte) reduces memory bottlenecks and improves performance!

---

## Performance Analysis

### Profiling Your Own Code

Use the common profiling infrastructure:

```bash
# Profile Python example
../../common/profiling/profile_pytorch.sh ./performance_basics.py

# View timeline in Nsight Systems
nsys-ui ../../results/ch1/performance_basics_pytorch_*.nsys-rep
```

**What to look for**:
- ERROR: Long CPU gaps between GPU kernels → Add async operations
- ERROR: Many small kernel launches → Batch or fuse operations
- ERROR: `aten::empty_strided` taking significant time → Preallocate buffers
- [OK] GPU utilization > 80% → Good!

### Expected Performance Improvements

| Optimization | Baseline → Optimized | Speedup |
|--------------|---------------------|---------|
| Preallocated buffers | 210ms overhead → 0ms | ~2x |
| Pinned memory | System dependent | 2-6x |
| CUDA Graphs | 5-20μs/launch → <1μs | 1.5-2x |
| Larger batches | 87 MFLOPs → 1000+ | 10x+ |
| **Combined** | **Overall end-to-end** | **5-10x** |

*Your results may vary by hardware.*

---

## Baseline/Optimized Example Pairs

All examples follow the `baseline_*.py` / `optimized_*.py` pattern and integrate with the benchmarking framework:

### Available Pairs

1. **Coalescing** (`baseline_coalescing.py` / `optimized_coalescing.py`)
   - Demonstrates coalesced vs uncoalesced memory access patterns
   - Shows bandwidth improvements from proper memory access

2. **Double Buffering** (`baseline_double_buffering.py` / `optimized_double_buffering.py`)
   - Overlaps memory transfer and computation using CUDA streams
   - Demonstrates latency hiding through async operations

**Run comparisons:**
```bash
python3 compare.py  # Compares all baseline/optimized pairs
```

---

## How to Run All Examples

```bash
cd ch1

# Install dependencies
pip install -r requirements.txt

# Run Python examples
python3 performance_basics.py                    # Baseline
python3 performance_basics_optimized.py          # Optimized comparisons
python3 roofline_analysis.py                     # Roofline model

# Run baseline/optimized comparisons
python3 compare.py                               # Compare all pairs

# Build and run CUDA examples
make
./batched_gemm_example_sm100                     # Batched GEMM
./arithmetic_intensity_demo_sm100                # Kernel optimization strategies

# Profile examples (optional)
../../common/profiling/profile_pytorch.sh ./performance_basics_optimized.py
../../common/profiling/profile_cuda.sh ./arithmetic_intensity_demo_sm100 ch1_ai
```

---

## Key Takeaways

1. **Always profile first**: Don't optimize blindly. Use profilers to identify actual bottlenecks.

2. **Memory management matters**: Preallocating buffers and using pinned memory can give 2-6x speedups with minimal code changes.

3. **Batch operations**: GPUs thrive on parallelism. Batching operations reduces overhead and improves efficiency dramatically (10x+ in many cases).

4. **Launch overhead is real**: For small operations, kernel launch overhead dominates. CUDA Graphs and batching mitigate this.

5. **Compound improvements**: Individual optimizations multiply. A 2x + 2x + 1.5x → 6x combined speedup.

6. **Low-hanging fruit**: These optimizations require minimal code changes but deliver major improvements. Always apply them first!

---

## Common Pitfalls

### Pitfall 1: Over-batching
**Problem**: Batch size too large → OOM (out of memory) errors.

**Solution**: Find sweet spot using batch size sweep (shown in `performance_basics_optimized.py`). Typical range: 64-512 for modern NVIDIA GPUs.

### Pitfall 2: CUDA Graphs with Dynamic Shapes
**Problem**: CUDA Graphs require static shapes. Dynamic models will fail or show no speedup.

**Solution**: Only use graphs for static portions of your model. Prefill/decode in inference are good candidates.

### Pitfall 3: Measuring Without Synchronization
**Problem**: CUDA operations are async. `time.time()` without `torch.cuda.synchronize()` measures queue time, not execution time!

**Solution**: Always synchronize before timing:
```python
torch.cuda.synchronize()
start = time.time()
model(input)
torch.cuda.synchronize()  # Critical!
elapsed = time.time() - start
```

### Pitfall 4: Cold Start Measurements
**Problem**: First few iterations include GPU warmup, driver overhead, cuDNN autotuning.

**Solution**: Always warmup (10-20 iterations) before benchmarking.

---

## Next Steps

**Ready for more?** → [Chapter 2: GPU Hardware Architecture](../ch2/README.md)

Learn about:
- NVIDIA GPU hardware architecture
- Memory hierarchy
- NVLink and interconnects
- How hardware architecture informs optimization strategy

**Want to dive deeper into profiling?** → [Chapter 13: PyTorch Profiling](../ch13/README.md)

---

## Additional Resources

- **Official Docs**: [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- **cuBLAS Documentation**: [CUDA Toolkit Docs - cuBLAS](https://docs.nvidia.com/cuda/cublas/)
- **CUDA Graphs**: [CUDA Programming Guide - Graphs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs)

---

**Chapter Status**: [OK] Complete

