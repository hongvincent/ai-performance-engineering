# Chapter 19: FP4/FP6/FP8 Training and Quantization

## Overview

Low-precision training with FP4, FP6, and FP8 enables faster training and inference while reducing memory usage. This chapter covers NVIDIA's hardware-accelerated low-precision formats on NVIDIA GPU NVIDIA GPU/B300 GPUs, quantization techniques, dynamic precision switching, and production deployment patterns.

## Learning Objectives

After completing this chapter, you can:

- [OK] Implement FP4/FP6/FP8 quantization for training and inference
- [OK] Use dynamic precision switching for accuracy/performance tradeoffs
- [OK] Apply FP8 with Transformer Engine for production training
- [OK] Measure and optimize quantization performance
- [OK] Choose appropriate precision for different model components
- [OK] Deploy quantized models with 2-7x speedups

## Prerequisites

**Previous chapters**:
- [Chapter 10: Tensor Cores](../ch10/README.md) - matrix operations with tensor cores
- [Chapter 16: Inference Optimization](../ch16/README.md) - FP8 inference basics

**Required**: NVIDIA GPU NVIDIA GPU/B300 GPU (SM 10.0) or NVIDIA GPU (SM 12.1), PyTorch 2.9+, CUDA 13.0+

---

## Low-Precision Formats

### Format Comparison

| Precision | Bits | TFLOPS (NVIDIA GPU) | Memory vs FP16 | Best For |
|-----------|------|---------------|----------------|----------|
| **FP4 (E2M1)** | 4 | ~1600 | 75% savings (4x) | Draft models, speculative decoding |
| **FP6 (E3M2)** | 6 | ~1400 | 50% savings (2.67x) | Balanced accuracy/compression |
| **FP8 (E4M3)** | 8 | ~450 | 50% savings (2x) | Production training & inference |
| **FP16** | 16 | ~225 | Baseline | Standard precision |
| **FP32** | 32 | ~225 | 2x memory | Legacy/debugging |

### Format Details

#### FP4 (E2M1) - NVFP4
- **Exponent**: 2 bits → Range: ~[0.125, 3.5]
- **Mantissa**: 1 bit + implicit leading 1
- **Use case**: Maximum compression, ~25% quantization error acceptable
- **NVIDIA GPU feature**: Hardware microscaling support

#### FP6 (E3M2) - NVFP6
- **Exponent**: 3 bits → Range: ~[0.03, 60]
- **Mantissa**: 2 bits + implicit leading 1
- **Use case**: Better accuracy than FP4 (~12.5% error), still high compression
- **NVIDIA GPU feature**: Native hardware support

#### FP8 (E4M3FN) - NVFP8
- **Exponent**: 4 bits → Range: ~[2^-9, 448]
- **Mantissa**: 3 bits + implicit leading 1
- **Use case**: Production training/inference with minimal accuracy loss
- **NVIDIA GPU feature**: Full tensor core acceleration

---

## Examples

### 1. `native_fp8_training.py` - Production FP8 Training

**Purpose**: Full production FP8 training pipeline with scaling management.

**Key features**:
- `FP8ScalingManager` for numerical stability
- Transformer Engine integration
- Automatic loss scaling
- Mixed FP8/FP16 training

**How to run**:
```bash
# Basic training
python native_fp8_training.py --epochs 10

# With profiling
nsys profile -o fp8_training --trace=cuda,nvtx python native_fp8_training.py

# Validate against FP16
python native_fp8_training.py --epochs 10 --validate-fp16
```

**Expected results** (8x NVIDIA GPU):
```
FP16 training: 50ms/iteration, 2048 MB memory
FP8 training:  28ms/iteration, 1024 MB memory
Speedup: 1.8x, Memory: 50% savings [OK]
Accuracy: <0.1% loss vs FP16
```

**Code structure**:
```python
from torch.cuda.amp import autocast
import torch.nn as nn

class FP8ScalingManager:
    """Manages scaling factors for FP8 training"""
    def __init__(self, init_scale=2**8):
        self.scale = init_scale
        self.growth_interval = 2000
    
    def update(self, grads_finite):
        if grads_finite:
            self.scale *= 1.01  # Grow slowly
        else:
            self.scale *= 0.5   # Shrink quickly on overflow

# Training loop
scaler = FP8ScalingManager()
for batch in dataloader:
    with autocast(dtype=torch.float8_e4m3fn):
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    loss.backward()
    scaler.update(check_finite(model.parameters()))
    optimizer.step()
```

**Performance on NVIDIA GPU**:
- **Training**: 1.8-2.0x faster than FP16
- **Memory**: 50% reduction
- **Accuracy**: <0.1% validation loss vs FP16
- **Scaling**: Linear to 8 GPUs

---

### 2. `native_fp4_quantization.py` - FP4 for Maximum Compression

**Purpose**: Implement FP4 (E2M1) for draft models and speculative decoding.

**Classes provided**:
- `FP4Tensor`: FP4 tensor with automatic dequantization
- `FP4Linear`: FP4 linear layer with optimized matmul
- `FP4MLP`: Complete FP4 multi-layer perceptron

**How to run**:
```bash
python native_fp4_quantization.py

# With benchmarking
python native_fp4_quantization.py --benchmark

# Profile
nsys profile -o fp4 --trace=cuda,nvtx python native_fp4_quantization.py
```

**Expected results** (NVIDIA GPU):
```
FP32: 0.139ms, 15.47 TFLOPS
FP16: 0.127ms, 16.86 TFLOPS
FP8:  0.073ms, 29.37 TFLOPS (1.9x vs FP32)
FP4:  ~0.04ms, ~54 TFLOPS (estimate, 3.8x vs FP32)
```

**Use cases**:
- [OK] Draft models for speculative decoding (7x more throughput)
- [OK] Multi-model serving (4x more models per GPU)
- [OK] Edge deployment (75% memory savings)
- ERROR: High-accuracy production (too much quantization error)

**Quantization error**:
- Typical: 20-30% L2 error on weights
- Acceptable for draft models where speed > accuracy
- Not suitable for final generation

---

### 3. `native_fp6_quantization.py` - FP6 Balanced Approach

**Purpose**: FP6 (E3M2) for better accuracy than FP4 with still-high compression.

**How to run**:
```bash
python native_fp6_quantization.py --benchmark
```

**Expected results**:
```
Memory savings: 50% (2.67x compression)
TFLOPS: ~1400 on NVIDIA GPU
Quantization error: ~12.5% (vs ~25% for FP4)
Speedup: ~6x vs FP32
```

**Use cases**:
- [OK] Models where FP4 is too lossy but FP8 insufficient compression
- [OK] Intermediate draft/production scenarios
- [OK] Balance between accuracy and memory

---

### 4. `fp8_compiled_matmul.py` - FP8 with torch.compile

**Purpose**: Combine FP8 with `torch.compile` for maximum performance.

**How to run**:
```bash
python fp8_compiled_matmul.py

# With profiling
ncu --set full -o fp8_compiled_ncu --launch-skip 5 --launch-count 10 python fp8_compiled_matmul.py
```

**Expected speedup**:
```
FP16 (eager):     3.8ms
FP16 (compiled):  3.2ms (1.2x)
FP8 (eager):      2.8ms (1.4x)
FP8 (compiled):   1.9ms (2.0x) [OK]
```

**Key optimizations**:
- Kernel fusion from `torch.compile`
- FP8 tensor core utilization
- Reduced memory bandwidth (half the data)

**Code**:
```python
@torch.compile(mode='max-autotune')
def fp8_matmul(A, B):
    A_fp8 = A.to(torch.float8_e4m3fn)
    B_fp8 = B.to(torch.float8_e4m3fn)
    return torch.matmul(A_fp8, B_fp8).to(torch.float16)
```

---

### 5. `dynamic_precision_switching.py` - Adaptive Precision

**Purpose**: Dynamically switch precision based on workload and accuracy requirements.

**Strategies**:
1. **Confidence-based**: High confidence → FP4, uncertain → FP16
2. **Layer-based**: Attention in FP8, FFN in FP4
3. **Token-based**: Important tokens in FP8, filler in FP4
4. **Memory-pressure**: FP8 normally, FP4 under memory pressure

**How to run**:
```bash
python dynamic_precision_switching.py --strategy confidence

# Profile switching overhead
nsys profile -o dynamic_precision --trace=cuda,nvtx python dynamic_precision_switching.py
```

**Example strategy**:
```python
class AdaptivePrecisionModel(nn.Module):
    def forward(self, x, confidence=None):
        if confidence is not None and confidence > 0.9:
            # High confidence: use FP4 for speed
            with autocast(dtype=torch.float8_e4m3fn):
                return self.model_fp4(x)
        else:
            # Low confidence: use FP8/FP16 for accuracy
            with autocast(dtype=torch.float8_e4m3fn):
                return self.model_fp8(x)
```

**Performance**:
- FP4 path: 7x faster, 5-10% accuracy loss
- FP8 path: 2x faster, <0.5% accuracy loss
- Adaptive: 4x average speedup, 2% accuracy loss

---

### 6. `token_precision_switching.py` - Per-Token Precision

**Purpose**: Switch precision per token based on importance.

**Strategy**:
```python
def forward_with_token_precision(self, input_ids, token_importance):
    # Important tokens (attention, special tokens): FP16/FP8
    important_mask = token_importance > threshold
    
    # Filler tokens (padding, common words): FP4
    filler_mask = ~important_mask
    
    # Process with different precisions
    important_output = self.process_fp8(input_ids[important_mask])
    filler_output = self.process_fp4(input_ids[filler_mask])
    
    return merge_outputs(important_output, filler_output)
```

**Use case**: Long context inference where most tokens don't need high precision.

**Expected**: 3-5x speedup for long sequences (>8K tokens)

---

### 7. `validate_quantization_performance.py` - Comprehensive Validation

**Purpose**: Automated validation framework with profiling and reporting.

**How to run**:
```bash
# Run all validations with profiling
python validate_quantization_performance.py --profile-all

# Single example
python validate_quantization_performance.py --example fp8_matmul --profile

# Generate report
python validate_quantization_performance.py --generate-report
```

**Output**:
```
./validation_results/
├── quantization_validation_report.md    # Comprehensive report
├── fp8_matmul_results.json              # Raw metrics
└── profiler_output/
    ├── FP8_Matmul_FP32_trace.json      # PyTorch profiler traces
    ├── FP8_Matmul_FP16_trace.json
    └── FP8_Matmul_FP8_trace.json
```

**Features**:
- [OK] NVTX markers for nsys integration
- [OK] Memory tracking (allocated, reserved, peak)
- [OK] TFLOPS calculation
- [OK] Speedup analysis
- [OK] Automated report generation

---

## Performance Analysis

### Expected Performance on NVIDIA GPU NVIDIA GPU

#### Transformer Layer (d=4096, ff=16384, seq=2048, batch=64)

| Precision | Time (ms) | Memory (MB) | Tokens/sec | Quality |
|-----------|-----------|-------------|------------|---------|
| FP32      | ~100      | 4096        | 1.3M       | Baseline |
| FP16      | ~50       | 2048        | 2.6M       | Baseline |
| **FP8**   | **~28**   | **1024**    | **4.6M**   | <0.1% loss |
| **FP6**   | **~32**   | **1024**    | **4.0M**   | ~1% loss |
| **FP4**   | **~15**   | **512**     | **8.5M**   | 5-10% loss |

#### GEMM Performance (M=N=K=4096)

| Precision | Time (ms) | TFLOPS | Memory (MB) | Speedup |
|-----------|-----------|--------|-------------|---------|
| FP32      | ~7.5      | 225    | 512         | 1.0x    |
| FP16      | ~3.8      | 450    | 256         | 2.0x    |
| **FP8**   | **~3.8**  | **450**| **128**     | **2.0x** |
| **FP4**   | **~1.1**  | **1600**| **64**     | **7.0x** |

### Measured Results (NVIDIA GPU, SM 12.1)

From `validate_quantization_performance.py`:
```
[OK] FP8 validation complete!
   Expected: 450 TFLOPS on NVIDIA GPU (vs 225 TFLOPS FP16)
   Actual FP32: 15.47 TFLOPS
   Actual FP16: 16.86 TFLOPS
   Actual FP8:  29.37 TFLOPS

Speedup Analysis:
  FP8 vs FP32: 1.90x faster
  FP8 throughput gain: 1.90x
```

*Note: NVIDIA GPU (SM 12.1) has different absolute performance than NVIDIA GPU (SM 10.0) but demonstrates FP8 speedup ratios*

---

## Use Cases by Precision

### FP4 (NVFP4)
**Best for**:
- [OK] Draft models for speculative decoding (7x speedup)
- [OK] Cost-optimized large-scale inference
- [OK] Edge deployment (75% memory savings)
- [OK] Multi-model serving (4x more models per GPU)

**Avoid for**:
- ERROR: High-accuracy production models
- ERROR: Training (too low precision for gradients)

### FP6 (NVFP6)
**Best for**:
- [OK] Balance between FP4 compression and FP8 accuracy
- [OK] Models where FP4 too lossy but FP8 insufficient compression
- [OK] Intermediate draft/production scenarios

### FP8 (NVFP8)
**Best for**:
- [OK] Production LLM training (1.8-2.0x speedup)
- [OK] Production inference with minimal accuracy loss
- [OK] Memory-constrained training (50% savings)
- [OK] High-throughput serving
- [OK] KV cache quantization

---

## How to Run All Examples

```bash
cd ch19

# Install dependencies
pip install torch>=2.9.0 numpy

# 1. Validate all precisions
python validate_quantization_performance.py --profile-all

# 2. FP4 examples
python native_fp4_quantization.py --benchmark

# 3. FP6 examples
python native_fp6_quantization.py --benchmark

# 4. FP8 training
python native_fp8_training.py --epochs 10

# 5. FP8 with compile
python fp8_compiled_matmul.py

# 6. Dynamic precision
python dynamic_precision_switching.py --strategy confidence

# 7. Token-level precision
python token_precision_switching.py

# Profile with nsys
nsys profile -o fp8_training --trace=cuda,nvtx,osrt,cudnn,cublas \
    python native_fp8_training.py

# Profile with ncu
ncu --set full -o fp8_ncu --launch-skip 5 --launch-count 10 \
    python fp8_compiled_matmul.py
```

---

## Key Takeaways

1. **FP8 is production-ready**: 2x speedup with <0.1% accuracy loss on NVIDIA GPU.

2. **Memory savings enable larger models**: 50% reduction allows 2x batch size or longer sequences.

3. **FP4 for draft models**: 7x speedup for speculative decoding where accuracy is secondary.

4. **Dynamic precision switching**: Adaptive strategies can achieve 4x average speedup.

5. **Tensor cores required**: These speedups only apply on NVIDIA GPU/Hopper GPUs with FP8 tensor cores.

6. **Scaling management critical**: FP8 training requires careful loss scaling to avoid under/overflow.

7. **Profile to validate**: Always measure actual speedup on your hardware and workload.

---

## Common Pitfalls

### Pitfall 1: Not Using Scaling with FP8
**Problem**: FP8 overflow/underflow without proper scaling → NaN losses.

**Solution**: Use `FP8ScalingManager` or Transformer Engine:
```python
scaler = FP8ScalingManager()
loss.backward()
scaler.update(check_finite(model.parameters()))
```

### Pitfall 2: Quantizing Everything to FP4
**Problem**: 25% quantization error on all layers → Poor quality.

**Solution**: Use FP4 only where acceptable (draft models, less critical layers).

### Pitfall 3: Not Checking Hardware Support
**Problem**: Running FP8 on GPU without tensor core support → Slow emulation.

**Check**:
```python
if torch.cuda.get_device_capability() < (9, 0):  # Hopper/NVIDIA GPU
    print("FP8 tensor cores not available!")
```

### Pitfall 4: Ignoring Memory Layout
**Problem**: Quantized tensors with poor memory layout → No speedup.

**Solution**: Use contiguous tensors and align to 128-byte boundaries:
```python
tensor = tensor.contiguous()
```

### Pitfall 5: Over-Quantizing KV Cache
**Problem**: Quantizing KV cache to FP4 → Attention accuracy degraded.

**Solution**: Use FP8 for KV cache (2x savings, <1% accuracy loss):
```python
k_cache = k_cache.to(torch.float8_e4m3fn)
v_cache = v_cache.to(torch.float8_e4m3fn)
```

---

## Profiling and Debugging

### PyTorch Profiler
```bash
python validate_quantization_performance.py --example fp8_matmul --profile
```

View Chrome trace: `chrome://tracing` → Load `profiler_output/FP8_Matmul_FP8_trace.json`

### NVIDIA Nsight Systems
```bash
nsys profile -o fp8_profile \
    --trace=cuda,nvtx,osrt,cudnn,cublas \
    --cuda-memory-usage=true \
    python native_fp8_training.py

# View
nsys-ui fp8_profile.nsys-rep
```

**Look for**:
- Tensor core utilization (should be >80%)
- Memory bandwidth (should be <50% for compute-bound)
- Launch overhead (should be minimal with FP8 batching)

### NVIDIA Nsight Compute
```bash
ncu --set full -o fp8_ncu \
    --target-processes all \
    --launch-skip 5 --launch-count 10 \
    python fp8_compiled_matmul.py

# Roofline analysis
ncu --set roofline -o fp8_roofline python fp8_compiled_matmul.py

# View
ncu-ui fp8_ncu.ncu-rep
```

---

## Next Steps

**Final chapter** → [Chapter 20: Putting It All Together](../ch20/README.md)

Learn about:
- End-to-end optimization workflows
- Real-world case studies combining FP8 + other techniques
- Production deployment patterns

**Related advanced topic** → [Chapter 18: Advanced Attention](../ch18/README.md)
- FlashAttention with FP8
- MLA (Multi-head Latent Attention) for reduced KV cache

---

## Additional Resources

- **NVIDIA Transformer Engine**: [GitHub](https://github.com/NVIDIA/TransformerEngine)
- **NVIDIA GPU Architecture**: [NVIDIA Whitepaper](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)
- **FP8 Training Guide**: [NVIDIA Developer Blog](https://developer.nvidia.com/blog/fp8-training)
- **Quantization Survey**: [arXiv:2103.13630](https://arxiv.org/abs/2103.13630)

### Additional Insights

**Weight-only quantization** (GPTQ, AWQ):
- Activation quantization (SmoothQuant)
- KV cache compression with FP4/FP8
- FP4 dynamic range and scaling
- Memory savings: FP16 weights + FP8 activations → 50%, INT4 weights + FP4 activations → 20%

**Dynamic precision strategies**:
- Dynamic precision strategy based on confidence
- KV cache: FP8 normally, FP4 under memory pressure
- Compute-limited: FP8 achieves 2× speedup
- Memory-bound: 1.5× achievable with FP8

---

**Chapter Status**: [OK] Complete

---

## Reference Materials

**For batched GEMM techniques** (cuBLAS batched operations, grouped GEMM for MoE), see `README_BATCHED_GEMM_REFERENCE.md` in this directory. While not directly related to FP8 training, batched operations can be combined with FP8 for maximum performance in multi-head attention and MoE layers.

