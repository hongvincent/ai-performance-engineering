""" Profiling Guide for Blackwell B200/B300 Optimizations ======================================================= This module provides comprehensive profiling workflows for Blackwell GPUs using Nsight Systems, Nsight Compute, and PyTorch Profiler. Tools Covered: 1. Nsight Systems - Timeline analysis 2. Nsight Compute - Kernel profiling 3. PyTorch Profiler - Python-level profiling 4. HTA (Holistic Tracing Analysis) - Distributed profiling Requirements: - CUDA 13.0+ - Nsight Systems 2025.2+ - Nsight Compute 2025.x+ - PyTorch 2.9+ Author: Blackwell Optimization Project """ import pathlib import sys _EXTRAS_REPO_ROOT = pathlib.Path(__file__).resolve().parents[2] if str(_EXTRAS_REPO_ROOT) not in sys.path: sys.path.insert(0, str(_EXTRAS_REPO_ROOT)) import os sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) import torch import torch.profiler as profiler import os import subprocess import csv import re from pathlib import Path from typing import Optional, List, Dict, Any # ============================================================================ # 1. Nsight Systems Profiling # ============================================================================ class NsightSystemsProfiler: """ Nsight Systems profiling for Blackwell Features: - Timeline visualization - CUDA API trace - Kernel execution timeline - Memory transfers - Multi-GPU traces Usage: profiler = NsightSystemsProfiler("my_trace") with profiler: # Your code here model(input) """ def __init__( self, output_name: str, trace_cuda: bool = True, trace_nvtx: bool = True, trace_cudnn: bool = True, trace_cublas: bool = True, ): self.output_name = output_name self.trace_cuda = trace_cuda self.trace_nvtx = trace_nvtx self.trace_cudnn = trace_cudnn self.trace_cublas = trace_cublas self._nvtx_pushed = False def __enter__(self): """Start profiling""" if torch.cuda.is_available(): try: torch.cuda.nvtx.range_push(f"Profile: {self.output_name}") self._nvtx_pushed = True except Exception: self._nvtx_pushed = False return self def __exit__(self, exc_type, exc_val, exc_tb): """Stop profiling""" if self._nvtx_pushed: try: torch.cuda.synchronize() except Exception: pass try: torch.cuda.nvtx.range_pop() except Exception: pass finally: self._nvtx_pushed = False @staticmethod def profile_command( script_path: str, output_name: str, duration: int = 30, *, ib_switch_guids: Optional[List[str]] = None, use_hardware_trace: bool = True, include_sqlite_export: bool = True, ) -> str: """ Generate nsys profile command Args: script_path: Path to Python script output_name: Output filename duration: Max duration in seconds ib_switch_guids: Optional InfiniBand switch GUIDs for metrics collection use_hardware_trace: Enable hardware-based CUDA tracing (requires Blackwell + Nsight Systems 2025.2+) include_sqlite_export: Append --export=sqlite for post-processing Returns: Command string to execute """ trace_domain = "cuda-hw,osrt,nvtx,ucx,gds" if use_hardware_trace else "cuda,osrt,nvtx,ucx,gds" cmd = [ "nsys", "profile", "-o", output_name, f"--trace={trace_domain}", "--trace-fork-before-exec=true", "--cuda-graph-trace=graph", "--cuda-event-trace=true", "--sample=cpu", "--gpu-metrics-devices=all", "--nic-metrics=true", "--storage-metrics", "--storage-devices=all", "--gds-metrics=driver", f"--duration={duration}", "--force-overwrite=true", ] if ib_switch_guids: cmd.append(f"--ib-switch-metrics-device={','.join(ib_switch_guids)}") if include_sqlite_export: cmd.append("--export=sqlite") cmd.extend(["python", script_path]) return " ".join(cmd) @staticmethod def analyze_blackwell_metrics(report_path: str): """ Analyze Blackwell-specific metrics from nsys report Key metrics: - SM utilization (target: >80% on 148 SMs) - Memory bandwidth (target: >7 TB/s for HBM3e) - Tensor Core utilization - NVLink bandwidth (if multi-GPU) """ print(f"=== Nsight Systems Analysis: {report_path} ===\n") print("Key Metrics to Check:") print("1. GPU Utilization:") print(" - Target: >80% on Blackwell's 148 SMs") print(" - Look for: GPU Utilization timeline") print("\n2. Memory Bandwidth:") print(" - Target: >7 TB/s (HBM3e peak: 7.8 TB/s)") print(" - Look for: Memory (HBM) Read/Write Throughput") print("\n3. Tensor Core Utilization:") print(" - Target: >70% for compute-bound workloads") print(" - Look for: Tensor Core Active cycles") print(" - Blackwell: Uses tcgen05.mma (not WGMMA)") print("\n4. Kernel Launch Overhead:") print(" - With CUDA graphs: <100 Î¼s") print(" - Without: ~10-50 Î¼s per launch") print("\n5. Multi-GPU (if applicable):") print(" - NVLink bandwidth: Target ~900 GB/s per link") print(" - NCCL operations: Check for overlapping") print("\nVisualization:") print(f" Open in Nsight Systems: nsys-ui {report_path}") try: NsightSystemsProfiler.summarize_report(report_path, print_summary=True) except Exception as exc: print(f"\n[warning] Unable to parse Nsight Systems report automatically: {exc}") # ------------------------------------------------------------------ # Automated stats extraction # ------------------------------------------------------------------ @staticmethod def summarize_report( report_path: str, *, kernel_regex: Optional[str] = None, top_k: int = 5, print_summary: bool = True, ) -> Dict[str, Any]: """ Parse an .nsys-rep file with `nsys stats` and optionally filter kernels. Args: report_path: Path to the `.nsys-rep` file (or `.qdrep`). kernel_regex: Optional regular expression to filter kernel names. top_k: Number of kernels to report (sorted by Time %). print_summary: If True, pretty-print results. Returns: Dictionary with top kernels and raw summary tables. """ report = Path(report_path) if not report.exists(): raise FileNotFoundError(f"Nsight Systems report not found: {report_path}") summary_rows = NsightSystemsProfiler._run_nsys_stats(report, "summary") kernel_rows = NsightSystemsProfiler._run_nsys_stats(report, "cuda_gpu_kern_sum") kernels = NsightSystemsProfiler._filter_and_rank_kernels( kernel_rows, kernel_regex, top_k ) result = { "report": str(report.resolve()), "summary": summary_rows, "kernels": kernels, } if print_summary: NsightSystemsProfiler._print_nsys_summary(result, kernel_regex) return result @staticmethod def _run_nsys_stats(report: Path, section: str) -> List[Dict[str, str]]: """ Execute `nsys stats` and parse CSV output for the requested section. """ cmd = [ "nsys", "stats", "--format", "csv", "--report", section, "--force-export", "true", str(report), ] try: proc = subprocess.run( cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, ) except subprocess.CalledProcessError as exc: raise RuntimeError( f"Failed to run {' '.join(cmd)}:\n{exc.stderr.strip()}" ) from exc csv_data: List[Dict[str, str]] = [] headers: Optional[List[str]] = None for raw_line in proc.stdout.splitlines(): stripped = raw_line.strip() if not stripped or raw_line.startswith("#"): headers = None continue if stripped.startswith("NOTICE") or stripped.startswith("Processing") or stripped.startswith("Generating"): continue row = next(csv.reader([raw_line])) if len(row) <= 1: headers = None continue if headers is None: if not (row[0].startswith("Time") or row[0].startswith("Metric")): continue headers = row continue if len(row) < len(headers): row.extend([""] * (len(headers) - len(row))) elif len(row) > len(headers): row = row[: len(headers)] csv_data.append({headers[i]: row[i] for i in range(len(headers))}) return csv_data @staticmethod def _filter_and_rank_kernels( kernel_rows: List[Dict[str, str]], kernel_regex: Optional[str], top_k: int, ) -> List[Dict[str, str]]: """ Filter kernel rows by regex and rank by Time (%). """ rows = kernel_rows if kernel_regex: pattern = re.compile(kernel_regex) rows = [row for row in rows if pattern.search(row.get("Name", ""))] def time_percent(row: Dict[str, str]) -> float: value = row.get("Time (%)") or row.get("Time (%) [sum]") try: return float(value.replace('"', "").strip().rstrip("%")) except (AttributeError, TypeError, ValueError): return 0.0 ranked = sorted(rows, key=time_percent, reverse=True) return ranked[:top_k] @staticmethod def _print_nsys_summary(summary: Dict[str, Any], kernel_regex: Optional[str]) -> None: """ Pretty-print the parsed Nsight Systems summary. """ print(f"\n=== Nsight Systems Summary: {summary['report']} ===") total_time = None for row in summary["summary"]: if "Elapsed Time (ns)" in row: total_time = float(row["Elapsed Time (ns)"]) break if total_time: print(f"Total captured time: {total_time / 1e6:.3f} ms") print("\nTop CUDA Kernels:") if kernel_regex: print(f" Filter: {kernel_regex}") kernels = summary["kernels"] if not kernels: print(" (No kernel entries matched filter)") return for idx, row in enumerate(kernels, 1): time_pct = (row.get("Time (%)") or row.get("Time (%) [sum]", "0")).replace('"', "") time_ns = ( row.get("Total Time (ns)") or row.get("Time (ns)") or row.get("Total Time (ns) [sum]") or row.get("Time (ns) [sum]", "0") ).replace('"', "") name = row.get("Name", "Unknown") try: time_ms = float(time_ns) / 1e6 except (TypeError, ValueError): time_ms = 0.0 try: pct = float(time_pct) except (TypeError, ValueError): pct = 0.0 print(f" {idx:>2}. {name}") print(f" Time: {time_ms:.3f} ms Share: {pct:.2f}%") # ============================================================================ # 2. Nsight Compute Profiling # ============================================================================ class NsightComputeProfiler: """Helpers for Nsight Compute profiling on Blackwell SM 10.0. Focus areas: - Core SM throughput (`sm__throughput.avg.pct_of_peak_sustained_elapsed`) - HBM3e bandwidth (`dram__throughput.avg.pct_of_peak_sustained_elapsed`) - tcgen05 tensor core utilization (`sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_elapsed`) - L2 cache efficiency (`l2_cache_hit_rate`) - Distributed shared memory and cluster launch metrics Usage: profiler = NsightComputeProfiler(kernel_name="my_kernel") profiler.profile_kernel(script_path="train.py") """ @staticmethod def profile_kernel_command( script_path: str, output_name: str, kernel_filter: Optional[str] = None, ) -> str: """ Generate ncu profile command Args: script_path: Path to Python script output_name: Output filename kernel_filter: Kernel name filter (e.g., "matmul") Returns: Command string """ metrics = [ "sm__throughput.avg.pct_of_peak_sustained_elapsed", # SM utilization "dram__throughput.avg.pct_of_peak_sustained_elapsed", # Memory BW "gpu__time_duration.sum", # Kernel duration "smsp__sass_thread_inst_executed_op_dfma_pred_on.sum", # Tensor Core ops "smsp__sass_thread_inst_executed_op_dmma_pred_on.sum", # MMA ops (Blackwell) ] cmd = [ "ncu", "--metrics", ",".join(metrics), "--kernel-name-base", "demangled", f"--export={output_name}", "--force-overwrite", ] if kernel_filter: cmd.extend(["--kernel-name", kernel_filter]) cmd.extend(["python", script_path]) return " ".join(cmd) @staticmethod def analyze_blackwell_kernel(report_path: str): """ Analyze Blackwell-specific kernel metrics """ print(f"=== Nsight Compute Analysis: {report_path} ===\n") print("Critical Metrics for Blackwell:") print("\n1. Compute Throughput:") print(" - FP8 Tensor Cores: Target >1000 TFLOPS") print(" - FP16: Target >600 TFLOPS") print(" - tcgen05.mma utilization: Target >70%") print("\n2. Memory Throughput:") print(" - HBM3e: Target >7 TB/s (>90% of 7.8 TB/s peak)") print(" - L2 Cache hit rate: Target >80%") print(" - Check for 256-byte burst access patterns") print("\n3. Warp Efficiency:") print(" - Active warps: Target >80%") print(" - Branch divergence: <10%") print(" - Memory coalescing: >90%") print("\n4. Occupancy:") print(" - Theoretical: Depends on kernel") print(" - Achieved: Target >75% of theoretical") print("\n5. Blackwell-Specific:") print(" - Thread Block Clusters: Check if used (up to 8 CTAs)") print(" - Distributed Shared Memory: Check DSMEM usage") print(" - TMA: Check async copy efficiency") print("\nVisualization:") print(f" Open in Nsight Compute: ncu-ui {report_path}") # ============================================================================ # 3. PyTorch Profiler Integration # ============================================================================ def profile_with_pytorch_profiler( fn: callable, output_dir: str = "./profiling_results", record_shapes: bool = True, profile_memory: bool = True, with_stack: bool = True, ): """ Profile with PyTorch's built-in profiler Features: - CPU and GPU time - Memory usage - Operator-level breakdown - TensorBoard integration Args: fn: Function to profile output_dir: Output directory record_shapes: Record tensor shapes profile_memory: Track memory usage with_stack: Include Python stack traces """ Path(output_dir).mkdir(parents=True, exist_ok=True) print(f"=== PyTorch Profiler ===") print(f"Output directory: {output_dir}\n") activities = [profiler.ProfilerActivity.CPU] include_cuda = False if torch.cuda.is_available(): try: torch.ones(1, device="cuda") torch.cuda.synchronize() activities.append(profiler.ProfilerActivity.CUDA) include_cuda = True except Exception as exc: # pragma: no cover - environment dependent print(f"WARNING: CUDA profiling disabled ({exc}); falling back to CPU-only profiling.") with profiler.profile( activities=activities, record_shapes=record_shapes, profile_memory=profile_memory, with_stack=with_stack, on_trace_ready=profiler.tensorboard_trace_handler(output_dir), schedule=profiler.schedule(wait=1, warmup=1, active=3, repeat=1), ) as prof: for _ in range(5): # warmup + active steps fn() prof.step() # Print summary print("\n" + "=" * 80) if include_cuda: print("Top 10 GPU operators:") print(prof.key_averages().table( sort_by="cuda_time_total", row_limit=10, )) else: print("Top 10 CPU operators:") print(prof.key_averages().table( sort_by="cpu_time_total", row_limit=10, )) print("\n" + "=" * 80) if include_cuda: print("Top 10 memory consumers (GPU):") print(prof.key_averages().table( sort_by="cuda_memory_usage", row_limit=10, )) else: print("Top 10 memory consumers (CPU):") print(prof.key_averages().table( sort_by="self_cpu_memory_usage", row_limit=10, )) print("\n" + "=" * 80) print(f"TensorBoard trace saved to: {output_dir}") print(f"View with: tensorboard --logdir={output_dir}") # ============================================================================ # 4. Complete Profiling Workflow # ============================================================================ def complete_profiling_workflow( model: torch.nn.Module, input_tensor: torch.Tensor, output_dir: str = "./profiling_blackwell", ): """ Complete profiling workflow for Blackwell Steps: 1. PyTorch profiler for high-level overview 2. Nsight Systems for timeline 3. Nsight Compute for kernel details Args: model: PyTorch model input_tensor: Input tensor output_dir: Output directory """ Path(output_dir).mkdir(parents=True, exist_ok=True) print("=" * 80) print("Complete Profiling Workflow for Blackwell") print("=" * 80) # 1. PyTorch Profiler print("\n Step 1: PyTorch Profiler (high-level overview)") def run_model(): with torch.no_grad(): _ = model(input_tensor) profile_with_pytorch_profiler( run_model, output_dir=f"{output_dir}/pytorch_profiler", ) # 2. Nsight Systems command print("\n\nStep 2: Nsight Systems (timeline analysis)") print("Run this command manually:") print("-" * 80) nsys_cmd = NsightSystemsProfiler.profile_command( "your_script.py", f"{output_dir}/nsys_trace", ) print(nsys_cmd) print("-" * 80) # 3. Nsight Compute command print("\n\nStep 3: Nsight Compute (kernel-level profiling)") print("Run this command manually:") print("-" * 80) ncu_cmd = NsightComputeProfiler.profile_kernel_command( "your_script.py", f"{output_dir}/ncu_report", ) print(ncu_cmd) print("-" * 80) print("\n\n" + "=" * 80) print("Profiling Workflow Complete!") print("=" * 80) print("\nNext Steps:") print("1. Review PyTorch Profiler in TensorBoard") print("2. Run Nsight Systems command for timeline") print("3. Run Nsight Compute command for kernel details") print("4. Compare against Blackwell targets:") print(" - SM utilization: >80%") print(" - Memory BW: >7 TB/s") print(" - Tensor Core util: >70%") print(" - FP8 TFLOPS: >1000") # ============================================================================ # 5. Quick Reference Commands # ============================================================================ def print_quick_reference(): """Print quick reference for profiling commands""" print("=" * 80) print("Blackwell Profiling Quick Reference") print("=" * 80) print("\n1. Nsight Systems (Timeline):") print(" nsys profile -o trace \\") print(" --trace=cuda-hw,osrt,nvtx,ucx,gds \\") print(" --trace-fork-before-exec=true --cuda-graph-trace=graph \\") print(" --cuda-event-trace=true --sample=cpu \\") print(" --gpu-metrics-devices=all --nic-metrics=true \\") print(" --storage-metrics --storage-devices=all \\") print(" --gds-metrics=driver python script.py") print(" # Fallback: replace cuda-hw with cuda when hardware trace is unavailable") print("\n2. Nsight Compute (Kernel Details):") print(" ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,\\") print(" dram__throughput.avg.pct_of_peak_sustained_elapsed \\") print(" --export=report python script.py") print("\n3. PyTorch Profiler (Python-level):") print(" # Use profile_with_pytorch_profiler() from this module") print("\n4. Key Blackwell Metrics:") print(" - SM utilization: >80% (148 SMs)") print(" - HBM3e bandwidth: >7 TB/s (>90% of 7.8 TB/s)") print(" - Tensor Core (tcgen05): >70% utilization") print(" - FP8: >1000 TFLOPS, FP16: >600 TFLOPS") print("\n5. Analysis Tools:") print(" - nsys-ui: Nsight Systems GUI") print(" - ncu-ui: Nsight Compute GUI") print(" - tensorboard: PyTorch Profiler visualization") print("=" * 80) # ============================================================================ # 6. Blackwell SM 10.0 Specific Metrics Guide # ============================================================================ class BlackwellMetricsGuide: """ Comprehensive guide to Blackwell SM 10.0 specific metrics in Nsight Compute. This class provides detailed information about Blackwell-specific counters, metrics, and best practices for profiling on B200/B300 GPUs. """ @staticmethod def get_essential_blackwell_metrics() -> List[str]: """ Returns list of essential Nsight Compute metrics for Blackwell SM 10.0. These metrics cover: - HBM3e memory performance - tcgen05 tensor core utilization - SM 10.0 compute throughput - L2 cache efficiency - Distributed Shared Memory (DSMEM) usage """ return [ # Core performance "sm__throughput.avg.pct_of_peak_sustained_elapsed", "sm__cycles_active.avg.pct_of_peak_sustained_elapsed", "gpu__time_duration.sum", "gpu__compute_memory_throughput.avg.pct_of_peak_sustained_elapsed", # HBM3e memory (7.8 TB/s peak) "dram__throughput.avg.pct_of_peak_sustained_elapsed", "dram_read_throughput", "dram_write_throughput", "dram_read_bytes", "dram_write_bytes", "lts__t_sectors_op_read.sum", # L2 read sectors (128B sectors for HBM3e) "lts__t_sectors_op_write.sum", # L2 write sectors # Blackwell Tensor Cores (tcgen05.mma) "sm__inst_executed_pipe_tensor_op.sum", # tcgen05 instructions "sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_elapsed", "smsp__inst_executed_pipe_tensor_op_hmma.sum", # Half-precision ops "smsp__inst_executed_pipe_tensor_op_qmma.sum", # FP8/FP6/FP4 ops "sm__mio_pq_read_cycles_active.avg.pct_of_peak_sustained_elapsed", # Cache efficiency (critical for HBM3e) "l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum", # L1 global loads "l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum", # L1 global stores "l2_cache_hit_rate", "l1tex__data_bank_conflicts_pipe_lsu.sum", # Warp efficiency "smsp__average_warps_issue_stalled.pct", "smsp__warps_launched.sum", "smsp__thread_inst_executed_per_inst_executed.ratio", # Occupancy "sm__warps_active.avg.pct_of_peak_sustained_active", "achieved_occupancy", # Thread Block Clusters (8 CTAs on Blackwell) "sm__ctas_launched.sum", "sm__maximum_warps_avg_per_active_cycle", ] @staticmethod def print_metric_guide(): """Print detailed guide for interpreting Blackwell metrics.""" print("=" * 80) print("Blackwell SM 10.0 Metrics Interpretation Guide") print("=" * 80) print("\n HBM3e Memory Metrics (7.8 TB/s peak, 178 GB capacity)") print("-" * 80) print("Metric: dram__throughput.avg.pct_of_peak_sustained_elapsed") print(" Target: >90% for memory-bound kernels") print(" >95%: Excellent - hitting HBM3e peak") print(" 80-90%: Good - near peak bandwidth") print(" <80%: Check memory access patterns\n") print("Metric: lts__t_sectors_op_read.sum / lts__t_sectors_op_write.sum") print(" Purpose: Track L2 sector accesses (128-byte sectors for HBM3e)") print(" Optimal: Aligned to 128-byte cache lines") print(" Check: (dram_bytes / l2_sectors) should be ~128 bytes\n") print("Memory Access Pattern Analysis:") print(" 256-byte bursts: Optimal for HBM3e (2x 128B cache lines)") print(" float4 vectorization: 16 bytes per load") print(" Cache streaming (.cs): Use for write-only patterns\n") print("\n tcgen05 Tensor Core Metrics (Blackwell 5th-gen)") print("-" * 80) print("Metric: sm__inst_executed_pipe_tensor_op.sum") print(" Purpose: Count tcgen05.mma instructions (NOT WGMMA!)") print(" >1M instructions/ms: Good tensor core usage") print(" Compare with total instructions for TC utilization ratio\n") print("Metric: sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_elapsed") print(" Target: >70% for matrix operations") print(" >80%: Excellent - tensor cores fully utilized") print(" 50-70%: Good - room for improvement") print(" <50%: Check tile sizes and data layout\n") print("Metric: smsp__inst_executed_pipe_tensor_op_qmma.sum") print(" Purpose: FP8/FP6/FP4 quantized tensor ops") print(" Blackwell specific: Native FP6/FP4 support") print(" FP8: ~1200 TFLOPS, FP6: ~1400 TFLOPS, FP4: ~1600 TFLOPS\n") print("\nðŸ”§ SM 10.0 Compute Metrics (148 SMs on B200)") print("-" * 80) print("Metric: sm__throughput.avg.pct_of_peak_sustained_elapsed") print(" Target: >80% for compute-bound kernels") print(" 148 SMs on B200: Massive parallelism") print(" Check: Launch enough blocks to saturate all SMs\n") print("Metric: achieved_occupancy") print(" Target: >0.5 (50%)") print(" Blackwell: 64 warps per SM (max)") print(" Balance: Occupancy vs register/shared memory usage\n") print("\nðŸ’¾ Cache Hierarchy (Blackwell)") print("-" * 80) print("L1 Cache: 256 KB per SM (shared with SMEM)") print(" Metric: l1tex__data_bank_conflicts_pipe_lsu.sum") print(" Target: 0 bank conflicts") print(" Optimize: Pad shared memory to avoid conflicts\n") print("L2 Cache: ~100 MB total") print(" Metric: l2_cache_hit_rate") print(" Target: >80% for reused data") print(" 128-byte cache lines optimized for HBM3e") print(" L2 promotion: Use CU_TENSOR_MAP_L2_PROMOTION_L2_128B\n") print("\n Performance Targets for Blackwell B200") print("-" * 80) print("FP8 Tensor Cores: ~1200 TFLOPS") print("FP16 Tensor Cores: ~600 TFLOPS") print("FP32: ~80 TFLOPS") print("HBM3e Bandwidth: 7.8 TB/s peak (aim for >7 TB/s)") print("NVLink 5.0: 1800 GB/s per GPU pair") print("SM Count: 148 SMs") print("Max Occupancy: 64 warps/SM * 148 SMs = 9,472 warps") print("\n" + "=" * 80) # ============================================================================ # 7. HBM3e Memory Pattern Analysis # ============================================================================ class HBM3eMemoryAnalyzer: """ Analyzer for HBM3e memory access patterns on Blackwell. HBM3e Characteristics: - 7.8 TB/s peak bandwidth - 128-byte cache lines - 256-byte optimal burst size - 178 GB capacity on B200 - 16 memory channels """ @staticmethod def analyze_memory_pattern( dram_read_throughput: float, dram_write_throughput: float, l2_read_sectors: int, l2_write_sectors: int, kernel_duration_ns: float, ) -> dict: """ Analyze HBM3e memory access pattern from Nsight Compute metrics. Args: dram_read_throughput: GB/s dram_write_throughput: GB/s l2_read_sectors: Number of L2 sectors read (128B each) l2_write_sectors: Number of L2 sectors written (128B each) kernel_duration_ns: Kernel duration in nanoseconds Returns: Dictionary with analysis results """ # Constants HBM3E_PEAK_BW = 7800 # GB/s SECTOR_SIZE = 128 # bytes OPTIMAL_BURST = 256 # bytes # Compute metrics total_throughput = dram_read_throughput + dram_write_throughput bw_utilization = (total_throughput / HBM3E_PEAK_BW) * 100 # Bytes transferred dram_read_bytes = dram_read_throughput * (kernel_duration_ns / 1e9) dram_write_bytes = dram_write_throughput * (kernel_duration_ns / 1e9) total_bytes = dram_read_bytes + dram_write_bytes # L2 sector analysis l2_read_bytes = l2_read_sectors * SECTOR_SIZE l2_write_bytes = l2_write_sectors * SECTOR_SIZE # Alignment analysis avg_bytes_per_read_sector = dram_read_bytes / l2_read_sectors if l2_read_sectors > 0 else 0 avg_bytes_per_write_sector = dram_write_bytes / l2_write_sectors if l2_write_sectors > 0 else 0 # Burst efficiency (how close to 256-byte optimal burst) read_burst_efficiency = min(avg_bytes_per_read_sector / OPTIMAL_BURST, 1.0) * 100 if avg_bytes_per_read_sector > 0 else 0 write_burst_efficiency = min(avg_bytes_per_write_sector / OPTIMAL_BURST, 1.0) * 100 if avg_bytes_per_write_sector > 0 else 0 # Classification if bw_utilization > 90: classification = "Excellent - Near HBM3e peak" elif bw_utilization > 70: classification = "Good - High HBM3e utilization" elif bw_utilization > 50: classification = "Moderate - Room for improvement" else: classification = "Low - Memory pattern issues likely" analysis = { 'bandwidth_utilization_pct': bw_utilization, 'total_throughput_gbps': total_throughput, 'dram_read_throughput_gbps': dram_read_throughput, 'dram_write_throughput_gbps': dram_write_throughput, 'total_bytes': total_bytes, 'avg_bytes_per_read_sector': avg_bytes_per_read_sector, 'avg_bytes_per_write_sector': avg_bytes_per_write_sector, 'read_burst_efficiency_pct': read_burst_efficiency, 'write_burst_efficiency_pct': write_burst_efficiency, 'classification': classification, 'aligned_to_128b': avg_bytes_per_read_sector >= 128 and avg_bytes_per_write_sector >= 128, 'optimal_256b_bursts': read_burst_efficiency > 90 and write_burst_efficiency > 90, } return analysis @staticmethod def print_optimization_recommendations(analysis: dict): """Print optimization recommendations based on analysis.""" print("\n" + "=" * 80) print("HBM3e Memory Optimization Recommendations") print("=" * 80) bw_util = analysis['bandwidth_utilization_pct'] print(f"\nCurrent Status: {analysis['classification']}") print(f"Bandwidth Utilization: {bw_util:.1f}% of 7.8 TB/s peak") if bw_util < 90: print("\nðŸ“ˆ Recommendations to improve HBM3e bandwidth:") if not analysis['aligned_to_128b']: print("\n1. ERROR: MEMORY ALIGNMENT ISSUE") print(" Problem: Not aligned to 128-byte cache lines") print(" Solution:") print(" - Use float4 (16B) or larger vectorized loads") print(" - Align data structures to 128-byte boundaries") print(" - Use __align__(128) for key data structures") print(" Example:") print(" __align__(128) float data[N];") print(" float4* data4 = reinterpret_cast<float4*>(data);") if not analysis['optimal_256b_bursts']: print("\n2. WARNING: BURST SIZE NOT OPTIMAL") print(" Problem: Not achieving 256-byte optimal bursts") print(" Solution:") print(" - Process 8x float4 (256 bytes) per thread") print(" - Use cache streaming modifiers (.cs)") print(" - Unroll loops for burst access") print(" Example:") print(" #pragma unroll") print(" for (int i = 0; i < 8; i++) {") print(" data[i] = __ldcs(&src[tid * 8 + i]); // 256B burst") print(" }") if bw_util < 50: print("\n3. ðŸ”´ LOW BANDWIDTH UTILIZATION") print(" Possible causes:") print(" - Insufficient parallelism (too few threads)") print(" - Memory access stride issues") print(" - Waiting for compute (not memory-bound)") print(" Solutions:") print(" - Increase grid size to saturate 148 SMs") print(" - Check occupancy (target >50%)") print(" - Profile to identify bottleneck") else: print("\n[OK] Excellent HBM3e utilization!") print(" Memory access pattern is well-optimized for Blackwell") print("\n" + "=" * 80) @staticmethod def print_hbm3e_best_practices(): """Print HBM3e optimization best practices.""" print("=" * 80) print("HBM3e Best Practices for Blackwell B200") print("=" * 80) print("\n1. ðŸ“ Alignment (128-byte cache lines)") print(" Align all data structures to 128 bytes") print(" Use __align__(128) attribute") print(" Ensure stride access is aligned") print("\n2. ðŸ“¦ Burst Access (256-byte optimal)") print(" Read/write 256 bytes per thread iteration") print(" Use 8x float4 (or 4x double4)") print(" Unroll loops for burst optimization") print("\n3. ðŸ”„ Vectorization") print(" Use float4/int4 for 16-byte loads") print(" Prefer vectorized over scalar loads") print(" Check compiler generates ld.global.v4") print("\n4. ðŸ’¨ Cache Streaming") print(" Use __ldcs() for streaming loads (bypass L1)") print(" Use __stcs() for streaming stores") print(" Good for write-only or read-once patterns") print("\n5. L2 Cache Optimization") print(" 128-byte cache lines match HBM3e") print(" Use CU_TENSOR_MAP_L2_PROMOTION_L2_128B") print(" Target >80% L2 hit rate for reused data") print("\n6. Parallelism") print(" Launch enough blocks for 148 SMs") print(" Each SM can handle multiple blocks") print(" Grid size >> 148 for load balancing") print("\n7. ðŸ” Profiling") print(" Use Nsight Compute to check:") print(" - dram__throughput (target >90%)") print(" - l2_cache_hit_rate (target >80%)") print(" - lts__t_sectors (check 128B alignment)") print("\n" + "=" * 80) print("\nCode Example - Optimal HBM3e Access:") print("-" * 80) print(""" __global__ void hbm3e_optimal_copy(const float4* __restrict__ src, float4* __restrict__ dst, size_t n) { size_t tid = blockIdx.x * blockDim.x + threadIdx.x; size_t stride = gridDim.x * blockDim.x; // Process 256 bytes per iteration (8x float4) for (size_t i = tid * 8; i < n; i += stride * 8) { #pragma unroll for (int j = 0; j < 8; j++) { // Cache streaming for write-only pattern float4 data = __ldcs(&src[i + j]); __stcs(&dst[i + j], data); } } } // Launch with enough blocks to saturate 148 SMs dim3 grid(148 * 4); // 4 blocks per SM dim3 block(512); // 512 threads per block hbm3e_optimal_copy<<<grid, block>>>(src, dst, n / 4); """) print("=" * 80) def run_complete_blackwell_analysis( nsys_report: str, ncu_report: str, ): """ Run complete Blackwell performance analysis. Combines Nsight Systems and Nsight Compute reports for comprehensive analysis. Args: nsys_report: Path to Nsight Systems report (.qdrep or .sqlite) ncu_report: Path to Nsight Compute report (.ncu-rep) """ print("=" * 80) print("Complete Blackwell B200 Performance Analysis") print("=" * 80) print("\n1. Timeline Analysis (Nsight Systems):") print("-" * 80) NsightSystemsProfiler.analyze_blackwell_metrics(nsys_report) print("\n2. Kernel Analysis (Nsight Compute):") print("-" * 80) NsightComputeProfiler.analyze_blackwell_kernel(ncu_report) print("\n3. Blackwell Specific Metrics:") print("-" * 80) BlackwellMetricsGuide.print_metric_guide() print("\n4. HBM3e Memory Optimization:") print("-" * 80) HBM3eMemoryAnalyzer.print_hbm3e_best_practices() print("\n" + "=" * 80) print("Analysis Complete") print("=" * 80) print("\nNext Steps:") print(" 1. Review bandwidth utilization (target >90%)") print(" 2. Check tensor core utilization (target >70%)") print(" 3. Verify memory access patterns (256-byte bursts)") print(" 4. Optimize based on bottleneck identified") print(" 5. Re-profile and compare improvements") # ============================================================================ # Example Usage # ============================================================================ if __name__ == "__main__": print("=== Blackwell Profiling Guide ===\n") def _select_execution_device() -> torch.device: if not torch.cuda.is_available(): print(" WARNING: CUDA not available â€” running example on CPU") return torch.device("cpu") try: torch.cuda.current_device() torch.ones(1, device="cuda") torch.cuda.synchronize() device_name = torch.cuda.get_device_name(0) print(f"GPU: {device_name}") return torch.device("cuda") except Exception as exc: print(f" WARNING: Unable to acquire CUDA device ({exc}); running example on CPU") return torch.device("cpu") exec_device = _select_execution_device() # Print quick reference print_quick_reference() print("\n\n" + "=" * 80) print("Example: Profile a simple model") print("=" * 80) # Create simple model model = torch.nn.Sequential( torch.nn.Linear(1024, 4096), torch.nn.GELU(), torch.nn.Linear(4096, 1024), ).to(exec_device) # Test input x = torch.randn(32, 1024, device=exec_device) # Profile with PyTorch profiler def run_model(): with torch.no_grad(): _ = model(x) profile_with_pytorch_profiler( run_model, output_dir="./example_profiling", ) print("\n Profiling example complete!") print("Check ./example_profiling for results") 