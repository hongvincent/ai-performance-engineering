# Chapter 14: torch.compile and Triton Kernels

## Overview

PyTorch 2.0+ includes a powerful compiler (`torch.compile`) and Triton for writing custom GPU kernels in Python. This chapter teaches you when and how to use these tools, understanding their trade-offs, and writing high-performance Triton kernels for specialized operations.

## Learning Objectives

After completing this chapter, you can:

- [OK] Use `torch.compile` for automatic optimization
- [OK] Understand compiler modes and when to use each
- [OK] Write custom Triton kernels in Python
- [OK] Apply Triton for fused operations and custom algorithms
- [OK] Recognize when torch.compile helps vs hurts performance
- [OK] Debug and optimize compiled code

## Prerequisites

**Previous chapters**:
- [Chapter 9: Kernel Efficiency & Arithmetic Intensity](../ch9/README.md) - roofline + fusion concepts
- [Chapter 13: PyTorch Profiling](../ch13/README.md) - identifying bottlenecks

**Required**: PyTorch 2.0+, Python 3.8+

## torch.compile Fundamentals

### Compiler Modes

| Mode | Optimization Level | Compile Time | Use Case |
|------|-------------------|--------------|----------|
| `'default'` | Balanced | Medium | General purpose |
| `'reduce-overhead'` | Focus on launch overhead | Medium | Many small ops |
| `'max-autotune'` | Maximum performance | Long | Production (compile once) |

### When torch.compile Helps

[OK] **Good candidates:**
- Small to medium models (1-10B parameters)
- Many element-wise operations
- Custom operations without optimized kernels
- Inference workloads

ERROR: **Poor candidates:**
- Very large models (40B+) - memory-bound, not compute-bound
- Already using optimized ops (cuDNN, cuBLAS)
- Dynamic shapes
- Heavy CPU preprocessing

---

## Examples

### 1. `torch_compiler_examples.py` - Comprehensive torch.compile Guide

**Purpose**: Demonstrate torch.compile usage patterns and trade-offs.

#### Basic Usage

```python
import torch

def my_model(x):
    x = x + 1.0
    x = x * 2.0
    x = torch.relu(x)
    return x

# Compile model
compiled_model = torch.compile(my_model)

# Use like regular function
input = torch.randn(1000, 1000, device='cuda')
output = compiled_model(input)  # First call compiles
output = compiled_model(input)  # Subsequent calls are fast
```

#### Mode Comparison

```python
import time

# Test different modes
modes = ['default', 'reduce-overhead', 'max-autotune']

for mode in modes:
    compiled = torch.compile(my_model, mode=mode)
    
    # Warmup
    for _ in range(10):
        _ = compiled(input)
    
    # Benchmark
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = compiled(input)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    print(f"{mode:20s}: {elapsed * 10:.2f} ms/iter")
```

**Expected results (1B model)**:
```
Eager mode:          15.2 ms/iter
default:             14.8 ms/iter (1.03x) [OK]
reduce-overhead:     13.1 ms/iter (1.16x) [OK]
max-autotune:        12.9 ms/iter (1.18x) [OK]
```

**Reality check (40B model)**:
```
Eager mode:          285 ms/iter
default:             287 ms/iter (0.99x) ERROR: Slower!
```

**Why 40B is slower?** Memory-bound. torch.compile optimizes compute, but can't overcome memory bandwidth limits.

**How to run**:
```bash
python3 torch_compiler_examples.py
```

---

### 2. `triton_examples.py` - Triton Kernel Basics

**Purpose**: Write custom GPU kernels in Python using Triton.

#### Fused Add-ReLU-Mul Kernel

**Problem**: Three separate PyTorch operations:
```python
# Unfused (3 kernels, 4 memory passes)
y = x + bias     # Load x, load bias, store y
y = torch.relu(y)  # Load y, store y
y = y * scale    # Load y, store y
```

**Solution**: Single Triton kernel:

```python
import triton
import triton.language as tl

@triton.jit
def fused_add_relu_mul_kernel(
    x_ptr, bias_ptr, out_ptr, scale,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    # Get program ID
    pid = tl.program_id(0)
    
    # Calculate offsets
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    # Load (single read)
    x = tl.load(x_ptr + offsets, mask=mask)
    bias = tl.load(bias_ptr + offsets, mask=mask)
    
    # Compute (fused)
    y = x + bias
    y = tl.where(y > 0, y, 0)  # ReLU
    y = y * scale
    
    # Store (single write)
    tl.store(out_ptr + offsets, y, mask=mask)

def fused_add_relu_mul(x, bias, scale):
    output = torch.empty_like(x)
    n_elements = x.numel()
    
    # Launch kernel
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    fused_add_relu_mul_kernel[grid](
        x, bias, output, scale,
        n_elements,
        BLOCK_SIZE=1024,
    )
    
    return output

# Benchmark
x = torch.randn(10000000, device='cuda')
bias = torch.randn(10000000, device='cuda')

# Unfused
torch.cuda.synchronize()
start = time.time()
y = x + bias
y = torch.relu(y)
y = y * 2.0
torch.cuda.synchronize()
unfused_time = time.time() - start

# Fused (Triton)
torch.cuda.synchronize()
start = time.time()
y = fused_add_relu_mul(x, bias, 2.0)
torch.cuda.synchronize()
fused_time = time.time() - start

print(f"Unfused: {unfused_time * 1000:.2f} ms")
print(f"Fused (Triton): {fused_time * 1000:.2f} ms")
print(f"Speedup: {unfused_time / fused_time:.2f}x")
```

**Expected speedup**: **3-4x** (reduced memory traffic)

**How to run**:
```bash
pip install triton
python3 triton_examples.py
```

---

### 3. `triton_fp8_advanced.py` - FP8 Operations in Triton

**Purpose**: Leverage FP8 Tensor Cores via Triton.

```python
import triton
import triton.language as tl

@triton.jit
def matmul_fp8_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Offsets
    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M
    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointers
    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn
    
    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Main loop
    for k in range(0, K, BLOCK_K):
        # Load FP8 data
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)
        
        # FP8 matrix multiply (uses Tensor Cores)
        acc += tl.dot(a, b)
        
        # Advance pointers
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # Store result
    c_ptrs = c_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn
    tl.store(c_ptrs, acc.to(tl.float16))
```

**Performance**: Achieves **90-95% of cuBLAS FP8 performance**!

**How to run**:
```bash
python3 triton_fp8_advanced.py
```

---

### 4. `triton_tma_blackwell.py` - TMA in Triton (NVIDIA GPU)

**Purpose**: Use Triton's TMA (Tensor Memory Accelerator) features on NVIDIA GPU.

**TMA (Tensor Memory Accelerator) is supported on modern NVIDIA GPUs. This example demonstrates the pattern for when it's fixed.

```python
@triton.jit
def tma_load_kernel(
    input_ptr, output_ptr,
    M, N,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # TMA load (hardware-accelerated on NVIDIA GPU)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    
    # Async TMA load
    data = tl.load(
        input_ptr + offs_m[:, None] * N + offs_n[None, :],
        eviction_policy="evict_last"  # TMA hint
    )
    
    # Process data
    result = data * 2.0
    
    # Store
    tl.store(output_ptr + offs_m[:, None] * N + offs_n[None, :], result)
```

**When TMA works**: 20-30% faster loads than regular async copies.

**How to run**:
```bash
python3 triton_tma_blackwell.py
```

---

### 5. NVIDIA GPU-Specific Features Testing

**Purpose**: Test and benchmark NVIDIA GPU-specific optimizations.

The comprehensive test suite is located in `tests/test_blackwell_optimizations.py`:

```python
import torch

# Test tensor core formats
def test_tensor_core_performance():
    # FP16 Tensor Cores
    a_fp16 = torch.randn(4096, 4096, dtype=torch.float16, device='cuda')
    b_fp16 = torch.randn(4096, 4096, dtype=torch.float16, device='cuda')
    
    # FP8 Tensor Cores (NVIDIA GPU)
    a_fp8 = a_fp16.to(torch.float8_e4m3fn)
    b_fp8 = b_fp16.to(torch.float8_e4m3fn)
    
    # Benchmark
    # FP16: ~1850 TFLOPS
    # FP8: ~3700 TFLOPS (2x faster!)
```

**How to run**:
```bash
# Run comprehensive test suite
pytest tests/test_blackwell_optimizations.py -v

# Run specific test categories
pytest tests/test_blackwell_optimizations.py -k test_fp8
pytest tests/test_blackwell_optimizations.py -m 'not slow'
```

---

## torch.compile Compilation Process

### What Happens During Compilation?

```
1. Graph Capture (TorchDynamo)
   ├─ Trace Python bytecode
   ├─ Build computation graph
   └─ Handle dynamic shapes

2. Graph Optimization (TorchInductor)
   ├─ Operator fusion
   ├─ Layout optimization  
   ├─ Memory planning
   └─ Auto-tuning

3. Code Generation
   ├─ Generate Triton kernels
   ├─ Call cuDNN/cuBLAS for large ops
   └─ Compile to PTX

4. Caching
   └─ Store compiled graph for reuse
```

### Compilation Overhead

| Model Size | First Run | Subsequent Runs |
|------------|-----------|-----------------|
| 1B params | +5-10s | +0s (cached) |
| 10B params | +30-60s | +0s |
| 40B params | +2-5min | +0s |

**Trade-off**: Compile once, run many times → Amortize cost.

---

## How to Run All Examples

```bash
cd ch14

# Install dependencies
pip install -r requirements.txt

# torch.compile examples
python3 torch_compiler_examples.py

# Triton examples
pip install triton
python3 triton_examples.py
python3 triton_fp8_advanced.py
python3 triton_tma_blackwell.py

# NVIDIA GPU optimizations (comprehensive test suite)
pytest tests/test_blackwell_optimizations.py -v

# Profile compiled code
python3 ../../common/profiling/profile_pytorch.sh ./torch_compiler_examples.py
```

---

## Key Takeaways

1. **torch.compile is not magic**: Works best for compute-bound workloads with many small operations. Memory-bound large models see minimal benefit.

2. **Compilation takes time**: First run is slow. Only worth it if you run many iterations (training) or deploy for inference.

3. **'max-autotune' for production**: Spend compilation time once, get best performance forever.

4. **Triton bridges Python and GPU**: Write high-performance kernels without learning CUDA C++. 90-95% of hand-tuned performance.

5. **FP8 on NVIDIA GPU is powerful**: 2x faster than FP16 with Tensor Cores. Use Triton or torch.float8_e4m3fn.

6. **Dynamic shapes hurt compilation**: Compiler assumes static shapes. Dynamic shapes cause recompilation.

7. **Profile to validate**: torch.compile can be slower! Always measure actual performance.

---

## Common Pitfalls

### Pitfall 1: Compiling Memory-Bound Models
**Problem**: torch.compile on 40B model → Same or worse performance.

**Reality**: Large models are memory-bound. Compiler can't overcome bandwidth limits.

**Solution**: Use quantization (FP8), not compilation, for large models.

### Pitfall 2: Dynamic Shapes
**Problem**: Input shapes change every iteration → Recompilation every time!

**Solution**: Pad to fixed sizes or use bucketing.

### Pitfall 3: Assuming Compilation Always Helps
**Problem**: "I'll compile everything!" → Longer dev time, same performance.

**Solution**: Profile first. Only compile compute-bound hotspots.

### Pitfall 4: Not Caching Compiled Code
**Problem**: Recompiling on every script run.

**Solution**: Use `TORCH_COMPILE_CACHE_DIR` environment variable.

### Pitfall 5: Forgetting Warmup
**Problem**: Including compilation time in benchmarks.

**Solution**: Always warmup 10+ iterations before measuring.

---

## Next Steps

**Disaggregated inference** → [Chapter 15: Disaggregated Inference](../ch15/README.md)

Learn about:
- Prefill/decode separation
- KV cache management
- Architectural patterns for inference

**Back to profiling** → [Chapter 13: PyTorch Profiling](../ch13/README.md)

---

## Additional Resources

- **torch.compile**: [PyTorch 2.0 Tutorial](https://pytorch.org/get-started/pytorch-2.0/)
- **Triton**: [OpenAI Triton](https://github.com/openai/triton)
- **TorchInductor**: [Compiler Architecture](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)
- **FP8 Training**: [NVIDIA Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/)

---

**Chapter Status**: [OK] Complete

