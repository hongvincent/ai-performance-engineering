# Chapter 11: CUDA Streams and Concurrency

## Overview

CUDA streams enable concurrent execution of independent operations, dramatically improving GPU utilization and throughput. This chapter teaches you how to use streams effectively, implement stream-ordered memory allocations, and build multi-stream pipelines that overlap computation and data transfer.

## Learning Objectives

After completing this chapter, you can:

- [OK] Create and manage CUDA streams for concurrent execution
- [OK] Overlap kernel execution, H2D, and D2H transfers
- [OK] Use stream-ordered allocators for zero-copy patterns
- [OK] Implement multi-stream pipelines for maximum throughput
- [OK] Measure and optimize stream concurrency
- [OK] Avoid common stream pitfalls (false dependencies, synchronization issues)

## Prerequisites

**Previous chapters**:
- [Chapter 6: CUDA Basics](../ch6/README.md) - kernel launches
- [Chapter 10: Pipelines](../ch10/README.md) - async patterns

**Required**: Understanding of asynchronous execution model

## Stream Fundamentals

### What are CUDA Streams?

**Stream**: A sequence of operations that execute in order on the GPU.

**Key property**: Operations in **different** streams can execute concurrently!

```
Default stream (synchronous):
[H2D] → [Kernel 1] → [Kernel 2] → [D2H]  (serial)

Multiple streams (async):
Stream 0: [H2D #0] → [Kernel #0] → [D2H #0]
Stream 1:     [H2D #1] → [Kernel #1] → [D2H #1]
Stream 2:          [H2D #2] → [Kernel #2] → [D2H #2]
          ↑ All can overlap!
```

**Typical speedup**: **2-3x** for independent operations.

---

## Examples

### 1. `basic_streams.cu` - Stream Basics

**Purpose**: Demonstrate fundamental stream operations and concurrency.

#### No Streams (Baseline)

```cpp
// All operations in default stream (serial)
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);
kernel<<<blocks, threads>>>(d_data);
cudaMemcpy(h_result, d_result, size, cudaMemcpyDeviceToHost);

// Timeline: [H2D] → [Kernel] → [D2H]  (serial)
// Time: 10ms + 20ms + 10ms = 40ms
```

#### With Streams (Optimized)

```cpp
const int NUM_STREAMS = 4;
cudaStream_t streams[NUM_STREAMS];

// Create streams
for (int i = 0; i < NUM_STREAMS; i++) {
    cudaStreamCreate(&streams[i]);
}

// Launch operations in different streams
for (int i = 0; i < NUM_STREAMS; i++) {
    int offset = i * chunk_size;
    
    // All async operations
    cudaMemcpyAsync(d_data + offset, h_data + offset, chunk_size,
                    cudaMemcpyHostToDevice, streams[i]);
    
    kernel<<<blocks, threads, 0, streams[i]>>>(d_data + offset);
    
    cudaMemcpyAsync(h_result + offset, d_result + offset, chunk_size,
                    cudaMemcpyDeviceToHost, streams[i]);
}

// Wait for all streams
for (int i = 0; i < NUM_STREAMS; i++) {
    cudaStreamSynchronize(streams[i]);
}

// Timeline (with overlap):
// Stream 0: [H2D #0] → [Kernel #0] → [D2H #0]
// Stream 1:     [H2D #1] → [Kernel #1] → [D2H #1]
// Stream 2:          [H2D #2] → [Kernel #2] → [D2H #2]
// Stream 3:               [H2D #3] → [Kernel #3] → [D2H #3]
//
// Time: max(H2D, Kernel, D2H) ≈ 20ms (vs 40ms)
// Speedup: 2x!
```

**How to run**:
```bash
make basic_streams
./basic_streams_sm100
```

**Expected output**:
```
Without streams: 42.3 ms
With 4 streams: 21.7 ms
Speedup: 1.95x [OK]
```

---

### 2. `stream_ordered_allocator.cu` - Stream-Ordered Memory

**Purpose**: Use stream-ordered allocations for zero-synchronization memory management.

**Problem with cudaMalloc**:
```cpp
cudaMalloc(&ptr, size);  // Synchronizes entire device!
// All streams blocked until allocation completes
```

**Solution with stream-ordered allocator**:
```cpp
// Allocate in stream (no global sync!)
cudaMallocAsync(&ptr, size, stream);

// Use immediately in same stream
kernel<<<blocks, threads, 0, stream>>>(ptr);

// Free in stream (deferred until kernel completes)
cudaFreeAsync(ptr, stream);

// No synchronization needed!
```

**Benefits**:
- [OK] No device-wide synchronization
- [OK] Deferred frees (safe even if kernels still running)
- [OK] Memory pool reuse (faster allocations)
- [OK] Better concurrency

**Example**:

```cpp
cudaStream_t stream;
cudaStreamCreate(&stream);

for (int iter = 0; iter < 100; iter++) {
    float *d_temp;
    
    // Allocate (async, no sync)
    cudaMallocAsync(&d_temp, size, stream);
    
    // Use
    process<<<blocks, threads, 0, stream>>>(d_temp);
    
    // Free (deferred, no sync)
    cudaFreeAsync(d_temp, stream);
}

cudaStreamSynchronize(stream);  // Single sync at end
```

**Speedup**: **3-5x** for workloads with frequent allocations.

**How to run**:
```bash
make stream_ordered_allocator
./stream_ordered_allocator_sm100
```

---

### 3. `warp_specialized_pipeline_multistream.cu` - Multi-Stream Pipeline

**Purpose**: Combine warp specialization (Ch10) with multi-stream execution for maximum throughput.

**Pattern**: Multiple streams, each with producer/consumer warps.

```cpp
const int NUM_STREAMS = 4;

__global__ void multi_stream_warp_specialized(
    float** inputs,   // Array of input pointers (one per stream)
    float** outputs,  // Array of output pointers
    int stream_id
) {
    int warp_id = threadIdx.x / 32;
    
    if (warp_id < 2) {
        // Producer warps: Load data for this stream
        load_data_async(inputs[stream_id], smem[stream_id]);
    } else {
        // Consumer warps: Process data
        process_data(smem[stream_id], outputs[stream_id]);
    }
}

// Launch multiple streams
for (int i = 0; i < NUM_STREAMS; i++) {
    multi_stream_warp_specialized<<<blocks, threads, 0, streams[i]>>>(
        inputs, outputs, i
    );
}
```

**Throughput**: **3-4x** higher than single-stream (perfect overlap).

**How to run**:
```bash
make warp_specialized_pipeline_multistream
./warp_specialized_pipeline_multistream_sm100
```

**CLI parameters**:

`warp_specialized_pipeline_multistream_sm100` now accepts runtime flags so you can sweep pipeline pressure without editing source:

```bash
./warp_specialized_pipeline_multistream_sm100 \
  --streams 4 \
  --batches 12 \
  --batch-elems 131072 \
  --release-threshold-gib 1.5
```

- `--streams <int>`: number of CUDA streams/buffers to rotate (default 3)
- `--batches <int>`: mini-batches to process (default 9)
- `--batch-elems <int>`: elements per batch (default 65,536)
- `--release-threshold-gib <float>`: stream-ordered allocator release threshold in GiB (default 2.0)
- `--skip-verify`: skip host-side correctness check for faster experimentation

Use `--help` to see the full flag list.

---

### 4. `warp_specialized_two_pipelines_multistream.cu` - Advanced Multi-Pipeline

**Purpose**: Run two independent pipelines concurrently using different streams.

**Use case**: 
- Pipeline 1: Inference prefill (large batch, low latency priority)
- Pipeline 2: Inference decode (small batch, high throughput priority)

```cpp
// Pipeline 1: Prefill (stream 0)
prefill<<<prefill_blocks, threads, 0, streams[0]>>>(
    tokens, kv_cache, attention_output
);

// Pipeline 2: Decode (stream 1) - concurrent!
decode<<<decode_blocks, threads, 0, streams[1]>>>(
    prev_tokens, kv_cache, next_token
);

// Both pipelines run simultaneously
// GPU efficiently switches between them
```

**How to run**:
```bash
make warp_specialized_two_pipelines_multistream
./warp_specialized_two_pipelines_multistream_sm100
```

---

## Stream Concurrency Patterns

### Pattern 1: Breadth-First Scheduling

**Goal**: Maximize concurrency by issuing all operations before waiting.

```cpp
// Good: Breadth-first
for (int i = 0; i < N; i++) {
    cudaMemcpyAsync(..., streams[i]);  // Issue all H2Ds
}
for (int i = 0; i < N; i++) {
    kernel<<<..., streams[i]>>>(...);  // Issue all kernels
}
for (int i = 0; i < N; i++) {
    cudaMemcpyAsync(..., streams[i]);  // Issue all D2Hs
}
// Maximum overlap!

// Bad: Depth-first
for (int i = 0; i < N; i++) {
    cudaMemcpyAsync(..., streams[i]);
    kernel<<<..., streams[i]>>>(...);
    cudaMemcpyAsync(..., streams[i]);
    // Waits for each stream to complete before next
}
```

### Pattern 2: Hyper-Q Exploitation

**NVIDIA GPU has 128 hardware queues**: Can truly execute 128 independent operations!

```cpp
const int NUM_STREAMS = 32;  // Exploit Hyper-Q
cudaStream_t streams[NUM_STREAMS];

// Create many streams
for (int i = 0; i < NUM_STREAMS; i++) {
    cudaStreamCreate(&streams[i]);
}

// Launch many small operations
for (int i = 0; i < 1000; i++) {
    small_kernel<<<1, 32, 0, streams[i % NUM_STREAMS]>>>(data[i]);
}
// GPU executes many in parallel!
```

### Pattern 3: Priority Streams

```cpp
cudaStream_t high_priority, low_priority;

// Create streams with priorities
int least_priority, greatest_priority;
cudaDeviceGetStreamPriorityRange(&least_priority, &greatest_priority);

cudaStreamCreateWithPriority(&high_priority, cudaStreamDefault, greatest_priority);
cudaStreamCreateWithPriority(&low_priority, cudaStreamDefault, least_priority);

// High-priority work preempts low-priority
latency_critical_kernel<<<..., high_priority>>>(...);  // Runs first
batch_processing<<<..., low_priority>>>(...);  // Yields to high-priority
```

---

## Performance Analysis

### Measuring Stream Overlap

Use Nsight Systems to visualize concurrency:

```bash
../../common/profiling/profile_cuda.sh ./basic_streams baseline
nsys-ui ../../results/ch11/basic_streams_*.nsys-rep
```

**Look for**:
- [OK] Overlapping kernel execution rows (concurrent streams)
- [OK] H2D during kernel execution (overlap)
- ERROR: Gaps between operations (missed opportunities)

### Stream Efficiency Metrics

| Configuration | Throughput | Stream Efficiency |
|---------------|------------|-------------------|
| No streams | 1.0x | 0% (serial) |
| 2 streams | 1.6x | 60% |
| 4 streams | 2.3x | 77% [OK] |
| 8 streams | 2.7x | 84% [OK] |
| 16 streams | 2.9x | 86% [OK] |

**Diminishing returns**: Beyond 8 streams, little benefit (overhead increases).

---

## How to Run All Examples

```bash
cd ch11

# Build all examples
make

# Run stream examples
./basic_streams_sm100                                      # Stream basics
./stream_ordered_allocator_sm100                           # Async allocation
./warp_specialized_pipeline_multistream_sm100              # Single pipeline (add flags, e.g. --streams 4)
./warp_specialized_two_pipelines_multistream_sm100         # Dual pipelines

# Profile to see concurrency
../../common/profiling/profile_cuda.sh ./basic_streams baseline

# View timeline
nsys-ui ../../results/ch11/basic_streams_*.nsys-rep
```

---

## Key Takeaways

1. **Streams enable concurrency**: Independent operations in different streams can overlap → 2-3x speedup.

2. **Stream-ordered allocations are faster**: `cudaMallocAsync` avoids device-wide sync → 3-5x faster for frequent allocations.

3. **Breadth-first scheduling maximizes overlap**: Issue all operations before waiting for any.

4. **Hyper-Q enables massive parallelism**: NVIDIA GPU has 128 hardware queues. Use 8-32 streams for best utilization.

5. **Priority streams for latency**: High-priority streams preempt low-priority → Better latency for critical work.

6. **Profile to validate**: Use Nsight Systems to see actual concurrency, not just hope for it.

7. **Diminishing returns after 8 streams**: More streams = more overhead. 8 is usually optimal.

---

## Common Pitfalls

### Pitfall 1: False Dependencies

**Problem**: Using default stream creates implicit dependencies.

```cpp
// Bad: All operations in default stream (serial)
kernel1<<<blocks, threads>>>(data);  // Default stream
kernel2<<<blocks, threads>>>(data);  // Default stream
// Serialized!

// Good: Explicit streams (concurrent if independent)
kernel1<<<blocks, threads, 0, stream1>>>(data1);
kernel2<<<blocks, threads, 0, stream2>>>(data2);
```

### Pitfall 2: Synchronization Too Early

**Problem**: Calling `cudaDeviceSynchronize()` before issuing all work.

```cpp
// Bad:
for (int i = 0; i < N; i++) {
    kernel<<<..., streams[i]>>>(...);
    cudaStreamSynchronize(streams[i]);  // Blocks here!
}

// Good:
for (int i = 0; i < N; i++) {
    kernel<<<..., streams[i]>>>(...);
}
for (int i = 0; i < N; i++) {
    cudaStreamSynchronize(streams[i]);  // Wait all at end
}
```

### Pitfall 3: Pinned Memory for Async Copies

**Problem**: Async `cudaMemcpyAsync` with pageable memory → Silently synchronizes!

**Solution**: Always use pinned memory:
```cpp
float *h_data;
cudaMallocHost(&h_data, size);  // Pinned
cudaMemcpyAsync(d_data, h_data, size, ..., stream);  // Truly async
```

### Pitfall 4: Too Many Streams

**Problem**: Creating 128 streams → Overhead dominates.

**Reality**: 8-16 streams is usually optimal. Beyond that, diminishing returns.

### Pitfall 5: Stream Leaks

**Problem**: Creating streams but never destroying them → Memory leak.

**Solution**: Always destroy streams:
```cpp
cudaStreamCreate(&stream);
// ... use stream ...
cudaStreamDestroy(stream);  // Don't forget!
```

---

## Next Steps

**CUDA Graphs for ultra-low latency** → [Chapter 12: CUDA Graphs](../ch12/README.md)

Learn about:
- Graph capture for repeatable workloads
- Conditional graphs
- Dynamic parallelism
- Sub-microsecond kernel launches

**Back to pipelines** → [Chapter 10: Tensor Cores and Pipelines](../ch10/README.md)

---

## Additional Resources

- **CUDA Streams**: [Programming Guide - Streams](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams)
- **Stream-Ordered Allocations**: [cudaMallocAsync Documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html)
- **Nsight Systems**: [Stream Analysis](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
- **Hyper-Q**: [Multi-Process Service](https://docs.nvidia.com/deploy/mps/index.html)

---

**Chapter Status**: [OK] Complete

