# LangChain & LangGraphë¥¼ í™œìš©í•œ AI ì—ì´ì „íŠ¸ ê°œë°œ

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [LangChain ê¸°ì´ˆ](#langchain-ê¸°ì´ˆ)
3. [LangGraph 1.0 ì†Œê°œ](#langgraph-10-ì†Œê°œ)
4. [ì—ì´ì „íŠ¸ íŒ¨í„´](#ì—ì´ì „íŠ¸-íŒ¨í„´)
5. [ì‹¤ì „ êµ¬í˜„](#ì‹¤ì „-êµ¬í˜„)
6. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

---

## ê°œìš”

**LangChain**ê³¼ **LangGraph**ëŠ” LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ìµœê³ ì˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### 2025ë…„ ìµœì‹  ì—…ë°ì´íŠ¸

- âœ¨ **LangChain 1.0 & LangGraph 1.0** ì •ì‹ ì¶œì‹œ
- ğŸ”’ **ì•ˆì •ì„± ë³´ì¥**: 2.0ê¹Œì§€ Breaking Changes ì—†ìŒ
- ğŸ“š **í†µí•© ë¬¸ì„œ**: docs.langchain.comì—ì„œ ëª¨ë“  ë¬¸ì„œ ì œê³µ
- ğŸš€ **Python 3.10+** ìš”êµ¬
- âš¡ **ì‹ ê·œ ê¸°ëŠ¥**: ë…¸ë“œ ìºì‹±, deferred nodes, pre/post hooks

---

## LangChain ê¸°ì´ˆ

### 1. ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install langchain langchain-openai

# LangGraph í¬í•¨
pip install langgraph

# ì „ì²´ ì„¤ì¹˜ (ê¶Œì¥)
pip install langchain langchain-openai langgraph langchain-community
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0.7
)

# ë©”ì‹œì§€ ì „ì†¡
messages = [
    SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    HumanMessage(content="Pythonì˜ ì¥ì ì„ ì•Œë ¤ì£¼ì„¸ìš”")
]

response = llm.invoke(messages)
print(response.content)
```

### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from langchain.prompts import ChatPromptTemplate

# í…œí”Œë¦¿ ì •ì˜
template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {subject} ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
    ("human", "{question}")
])

# ë©”ì‹œì§€ ìƒì„±
messages = template.format_messages(
    subject="Python í”„ë¡œê·¸ë˜ë°",
    question="ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë­”ê°€ìš”?"
)

response = llm.invoke(messages)
print(response.content)
```

### 4. Chain ì‚¬ìš©

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = PromptTemplate(
    input_variables=["product"],
    template="ë‹¤ìŒ ì œí’ˆì— ëŒ€í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”: {product}"
)

# Chain ìƒì„±
chain = LLMChain(llm=llm, prompt=prompt)

# ì‹¤í–‰
result = chain.run(product="AI ì±—ë´‡ í”Œë«í¼")
print(result)
```

---

## LangGraph 1.0 ì†Œê°œ

LangGraphëŠ” **ìƒíƒœë¥¼ ê°€ì§„ ë©€í‹° ì•¡í„° ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ LLMìœ¼ë¡œ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### í•µì‹¬ ê°œë…

1. **State (ìƒíƒœ)**: ê·¸ë˜í”„ ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” ë°ì´í„°
2. **Nodes (ë…¸ë“œ)**: ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
3. **Edges (ì—£ì§€)**: ë…¸ë“œ ê°„ì˜ ì—°ê²°
4. **Conditional Edges**: ì¡°ê±´ë¶€ ë¶„ê¸°

### ê¸°ë³¸ êµ¬ì¡°

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

# 1. ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    next_action: str

# 2. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def call_model(state: AgentState):
    """LLM í˜¸ì¶œ ë…¸ë“œ"""
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState):
    """ê³„ì† ì§„í–‰ ì—¬ë¶€ ê²°ì •"""
    last_message = state["messages"][-1]
    if "FINISH" in last_message.content:
        return "end"
    return "continue"

# 3. ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("agent", call_model)

# ì—£ì§€ ì¶”ê°€
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "agent",
        "end": END
    }
)

# ì»´íŒŒì¼
app = workflow.compile()

# ì‹¤í–‰
result = app.invoke({
    "messages": [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")]
})
```

---

## ì—ì´ì „íŠ¸ íŒ¨í„´

### 1. ReAct ì—ì´ì „íŠ¸ (Reasoning + Acting)

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool
from langchain_openai import ChatOpenAI

# ë„êµ¬ ì •ì˜
def search_tool(query: str) -> str:
    """ê²€ìƒ‰ ë„êµ¬ (ì‹œë®¬ë ˆì´ì…˜)"""
    return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼"

def calculator_tool(expression: str) -> str:
    """ê³„ì‚°ê¸° ë„êµ¬"""
    try:
        result = eval(expression)
        return f"ê²°ê³¼: {result}"
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

tools = [
    Tool(
        name="Search",
        func=search_tool,
        description="ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì…ë ¥ì€ ê²€ìƒ‰ ì¿¼ë¦¬ì…ë‹ˆë‹¤."
    ),
    Tool(
        name="Calculator",
        func=calculator_tool,
        description="ìˆ˜í•™ ê³„ì‚°ì„ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì…ë ¥ì€ Python ìˆ˜ì‹ì…ë‹ˆë‹¤."
    )
]

# ReAct í”„ë¡¬í”„íŠ¸
from langchain.prompts import PromptTemplate

react_prompt = PromptTemplate.from_template("""
ë‹¤ìŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{tools}

ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:

Question: ë‹µí•´ì•¼ í•  ì§ˆë¬¸
Thought: ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°
Action: ì‚¬ìš©í•  ë„êµ¬ [{tool_names}]
Action Input: ë„êµ¬ì— ì „ë‹¬í•  ì…ë ¥
Observation: ë„êµ¬ì˜ ê²°ê³¼
... (Thought/Action/Action Input/Observationì„ ë°˜ë³µ)
Thought: ì´ì œ ìµœì¢… ë‹µì„ ì•Œì•˜ìŠµë‹ˆë‹¤
Final Answer: ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€

ì‹œì‘!

Question: {input}
Thought: {agent_scratchpad}
""")

# ì—ì´ì „íŠ¸ ìƒì„±
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)
agent = create_react_agent(llm, tools, react_prompt)

# AgentExecutor
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=5
)

# ì‹¤í–‰
result = agent_executor.invoke({
    "input": "25 ê³±í•˜ê¸° 4ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
})
print(result["output"])
```

### 2. LangGraph ê¸°ë°˜ ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage
from typing import TypedDict, Annotated, Sequence
import operator

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[Sequence[HumanMessage | AIMessage], operator.add]

# ë„êµ¬ë¥¼ LangChain Toolë¡œ ë³€í™˜
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """íŠ¹ì • ìœ„ì¹˜ì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return f"{location}ì˜ ë‚ ì”¨: ë§‘ìŒ, 22Â°C"

@tool
def calculate(expression: str) -> float:
    """ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    return eval(expression)

tools = [get_weather, calculate]

# ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ
tool_node = ToolNode(tools)

# LLM ì„¤ì • (ë„êµ¬ ë°”ì¸ë”©)
llm_with_tools = llm.bind_tools(tools)

# ì—ì´ì „íŠ¸ ë…¸ë“œ
def call_agent(state: AgentState):
    """ì—ì´ì „íŠ¸ í˜¸ì¶œ"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

# ì¡°ê±´ë¶€ ì—£ì§€: ê³„ì† ë˜ëŠ” ì¢…ë£Œ
def should_continue(state: AgentState):
    """ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œì§€ í™•ì¸"""
    last_message = state["messages"][-1]

    # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ ê³„ì†
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"

    # ì—†ìœ¼ë©´ ì¢…ë£Œ
    return "end"

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_agent)
workflow.add_node("tools", tool_node)

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "end": END
    }
)

workflow.add_edge("tools", "agent")

# ì»´íŒŒì¼
app = workflow.compile()

# ì‹¤í–‰
response = app.invoke({
    "messages": [HumanMessage(content="ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?")]
})

for message in response["messages"]:
    print(f"{message.type}: {message.content}")
```

### 3. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Literal

# ìƒíƒœ ì •ì˜
class MultiAgentState(TypedDict):
    messages: list
    next_agent: str
    final_output: str

# ê° ì—ì´ì „íŠ¸ ë…¸ë“œ
def researcher(state: MultiAgentState):
    """ì—°êµ¬ ì—ì´ì „íŠ¸"""
    messages = state["messages"]
    prompt = f"ë‹¤ìŒ ì£¼ì œë¥¼ ì¡°ì‚¬í•˜ì„¸ìš”: {messages[-1]}"

    response = llm.invoke([HumanMessage(content=prompt)])

    return {
        "messages": messages + [f"[ì—°êµ¬ ê²°ê³¼] {response.content}"],
        "next_agent": "analyzer"
    }

def analyzer(state: MultiAgentState):
    """ë¶„ì„ ì—ì´ì „íŠ¸"""
    research_result = state["messages"][-1]
    prompt = f"ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì„¸ìš”: {research_result}"

    response = llm.invoke([HumanMessage(content=prompt)])

    return {
        "messages": state["messages"] + [f"[ë¶„ì„ ê²°ê³¼] {response.content}"],
        "next_agent": "writer"
    }

def writer(state: MultiAgentState):
    """ì‘ì„± ì—ì´ì „íŠ¸"""
    analysis_result = state["messages"][-1]
    prompt = f"ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”: {analysis_result}"

    response = llm.invoke([HumanMessage(content=prompt)])

    return {
        "messages": state["messages"] + [f"[ìµœì¢… ë³´ê³ ì„œ] {response.content}"],
        "next_agent": "end",
        "final_output": response.content
    }

# ë¼ìš°í„°: ë‹¤ìŒ ì—ì´ì „íŠ¸ ê²°ì •
def route_agent(state: MultiAgentState) -> Literal["researcher", "analyzer", "writer", "end"]:
    """ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…"""
    next_agent = state.get("next_agent", "researcher")

    if next_agent == "end":
        return END

    return next_agent

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(MultiAgentState)

workflow.add_node("researcher", researcher)
workflow.add_node("analyzer", analyzer)
workflow.add_node("writer", writer)

workflow.set_entry_point("researcher")

workflow.add_conditional_edges(
    "researcher",
    route_agent
)
workflow.add_conditional_edges(
    "analyzer",
    route_agent
)
workflow.add_conditional_edges(
    "writer",
    route_agent
)

# ì»´íŒŒì¼
multi_agent_app = workflow.compile()

# ì‹¤í–‰
result = multi_agent_app.invoke({
    "messages": ["Python ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°"],
    "next_agent": "researcher"
})

print("\n=== ìµœì¢… ê²°ê³¼ ===")
print(result["final_output"])
```

---

## ì‹¤ì „ êµ¬í˜„

### ì˜ˆì œ 1: RAG (Retrieval-Augmented Generation)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# 1. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
documents = [
    "Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
    "Pythonì€ ë°ì´í„° ê³¼í•™ê³¼ AI ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤."
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

splits = text_splitter.create_documents(documents)

# 2. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_documents(splits, embeddings)

# 3. RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2})
)

# ì‹¤í–‰
query = "Pythonì€ ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?"
result = qa_chain.invoke(query)
print(result["result"])
```

### ì˜ˆì œ 2: ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ëŒ€í™”í˜• ì—ì´ì „íŠ¸

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# ë©”ëª¨ë¦¬ í™œì„±í™”
memory = MemorySaver()

# ìƒíƒœ ì •ì˜
class ConversationState(TypedDict):
    messages: Annotated[list, operator.add]
    user_info: dict

def chatbot(state: ConversationState):
    """ëŒ€í™” ì²˜ë¦¬"""
    messages = state["messages"]
    user_info = state.get("user_info", {})

    # ì‚¬ìš©ì ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
    context = f"ì‚¬ìš©ì ì •ë³´: {user_info}\n\n"
    full_prompt = context + messages[-1].content

    response = llm.invoke([HumanMessage(content=full_prompt)])

    return {"messages": [response]}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(ConversationState)
workflow.add_node("chatbot", chatbot)
workflow.set_entry_point("chatbot")
workflow.add_edge("chatbot", END)

# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)

# ì„¸ì…˜ IDë¡œ ëŒ€í™” ê´€ë¦¬
config = {"configurable": {"thread_id": "user_123"}}

# ëŒ€í™” 1
result1 = app.invoke({
    "messages": [HumanMessage(content="ì œ ì´ë¦„ì€ í™ê¸¸ë™ì…ë‹ˆë‹¤")],
    "user_info": {"name": "í™ê¸¸ë™"}
}, config)

# ëŒ€í™” 2 (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ê¸°ì–µ)
result2 = app.invoke({
    "messages": [HumanMessage(content="ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?")]
}, config)

print(result2["messages"][-1].content)
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ìŠ¤íŠ¸ë¦¬ë°

```python
# LangGraph ìŠ¤íŠ¸ë¦¬ë°
for chunk in app.stream({
    "messages": [HumanMessage(content="ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”")]
}):
    print(chunk)
```

### 2. ë¹„ë™ê¸° ì‹¤í–‰

```python
import asyncio

async def async_agent():
    """ë¹„ë™ê¸° ì—ì´ì „íŠ¸"""
    result = await app.ainvoke({
        "messages": [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”")]
    })
    return result

# ì‹¤í–‰
# result = asyncio.run(async_agent())
```

### 3. ìºì‹± (ë…¸ë“œ ìºì‹±)

LangGraph 1.0ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì¸ ë…¸ë“œ ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•©ë‹ˆë‹¤.

```python
from langgraph.graph import StateGraph

# ìºì‹± í™œì„±í™” (ìë™)
workflow = StateGraph(AgentState)
# ë…¸ë“œ ìºì‹±ì€ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë¨
```

---

## ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### 1. Verbose ëª¨ë“œ

```python
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,  # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
    max_iterations=5
)
```

### 2. LangSmith (í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§)

```python
import os

# LangSmith ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# ì´ì œ ëª¨ë“  ì‹¤í–‰ì´ LangSmithì— ê¸°ë¡ë¨
```

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [LangChain ê³µì‹ ë¬¸ì„œ](https://docs.langchain.com/)
- [LangGraph ë¬¸ì„œ](https://docs.langchain.com/oss/python/langgraph/overview)
- [LangSmith](https://smith.langchain.com/)

---

**ì‘ì„±ì¼**: 2025-01-15
**ë²„ì „**: 1.0 (LangChain 1.0, LangGraph 1.0 ê¸°ë°˜)
**ë¼ì´ì„ ìŠ¤**: MIT
