# OpenAI APIë¥¼ í™œìš©í•œ AI ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´ë§

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [OpenAI API ê¸°ë³¸ ê°œë…](#openai-api-ê¸°ë³¸-ê°œë…)
3. [ìµœì‹  ëª¨ë¸ ê°€ì´ë“œ (2025)](#ìµœì‹ -ëª¨ë¸-ê°€ì´ë“œ-2025)
4. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
5. [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§](#í”„ë¡¬í”„íŠ¸-ì—”ì§€ë‹ˆì–´ë§)
6. [ë¹„ìš© ìµœì í™”](#ë¹„ìš©-ìµœì í™”)
7. [ì‹¤ì „ ì˜ˆì œ](#ì‹¤ì „-ì˜ˆì œ)

---

## ê°œìš”

OpenAIëŠ” GPT ì‹œë¦¬ì¦ˆë¥¼ ë¹„ë¡¯í•œ ìµœì²¨ë‹¨ AI ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ì„ ë„ì ì¸ AI ì—°êµ¬ ê¸°ê´€ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” **OpenAI APIë¥¼ í™œìš©í•˜ì—¬ ê³ ì„±ëŠ¥ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•**í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### 2025ë…„ ìµœì‹  ì—…ë°ì´íŠ¸

- âœ¨ **GPT-4.1 ì‹œë¦¬ì¦ˆ ì¶œì‹œ**: 1M í† í° ì»¨í…ìŠ¤íŠ¸, í–¥ìƒëœ ì½”ë”© ì„±ëŠ¥
- ğŸš€ **GPT-4.1 mini**: GPT-4o ëŒ€ë¹„ 83% ë¹„ìš© ì ˆê°, 50% ë ˆì´í„´ì‹œ ê°ì†Œ
- âš¡ **GPT-4.1 nano**: ê°€ì¥ ë¹ ë¥´ê³  ì €ë ´í•œ ì˜µì…˜
- ğŸ¯ **ì§€ì‹ ì»·ì˜¤í”„**: 2024ë…„ 6ì›”

---

## OpenAI API ê¸°ë³¸ ê°œë…

### 1. ì„¤ì¹˜ ë° ì„¤ì •

```bash
# OpenAI Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install openai

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë³´ì•ˆ í•„ìˆ˜!)
export OPENAI_API_KEY="your-api-key-here"
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from openai import OpenAI

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI()  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ ë¡œë“œ

# ë©”ì‹œì§€ ì „ì†¡
response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
    ]
)

print(response.choices[0].message.content)
```

### 3. ì£¼ìš” íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ | ë²”ìœ„ |
|---------|------|--------|------|
| `model` | ì‚¬ìš©í•  ëª¨ë¸ | gpt-4.1-mini | - |
| `messages` | ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ | í•„ìˆ˜ | - |
| `temperature` | ì‘ë‹µì˜ ì°½ì˜ì„± | 0.7 | 0.0-2.0 |
| `max_tokens` | ìµœëŒ€ ì¶œë ¥ í† í° | 1000 | 1-128000 |
| `top_p` | ëˆ„ì  í™•ë¥  ìƒ˜í”Œë§ | 1.0 | 0.0-1.0 |
| `frequency_penalty` | ë°˜ë³µ ì–µì œ | 0.0 | -2.0-2.0 |
| `presence_penalty` | ì£¼ì œ ë‹¤ì–‘ì„± | 0.0 | -2.0-2.0 |

---

## ìµœì‹  ëª¨ë¸ ê°€ì´ë“œ (2025)

### ëª¨ë¸ ë¹„êµí‘œ

| ëª¨ë¸ | ì»¨í…ìŠ¤íŠ¸ | ì£¼ìš” ìš©ë„ | ì„±ëŠ¥ | ë¹„ìš© | ì†ë„ |
|-----|---------|----------|------|------|------|
| **GPT-4.1** | 1M í† í° | ë³µì¡í•œ ì¶”ë¡ , ì½”ë”© | â­â­â­â­â­ | ë†’ìŒ | ì¤‘ê°„ |
| **GPT-4.1 mini** | 1M í† í° | ë²”ìš© ì‘ì—…, ë¹ ë¥¸ ì²˜ë¦¬ | â­â­â­â­ | ë‚®ìŒ | ë¹ ë¦„ |
| **GPT-4.1 nano** | 1M í† í° | ëŒ€ëŸ‰ ì²˜ë¦¬, ê°„ë‹¨í•œ ì‘ì—… | â­â­â­ | ë§¤ìš° ë‚®ìŒ | ë§¤ìš° ë¹ ë¦„ |
| **GPT-4o** | 128K í† í° | ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€, ì˜¤ë””ì˜¤) | â­â­â­â­ | ì¤‘ê°„ | ë¹ ë¦„ |
| **GPT-4o mini** | 128K í† í° | ê°„ë‹¨í•œ ì‘ì—…, ì €ë¹„ìš© | â­â­â­ | ë§¤ìš° ë‚®ìŒ | ë¹ ë¦„ |
| **GPT-3.5 Turbo** | 16K í† í° | ë ˆê±°ì‹œ ì§€ì› | â­â­ | ë‚®ìŒ | ë¹ ë¦„ |

### ê°€ê²© ì •ë³´ (2025ë…„ ê¸°ì¤€)

| ëª¨ë¸ | ì…ë ¥ (1M í† í°) | ì¶œë ¥ (1M í† í°) | ë¹„ìš© íš¨ìœ¨ì„± |
|-----|---------------|---------------|------------|
| GPT-3.5 Turbo | $0.50 | $1.50 | â­â­â­â­ |
| GPT-4o mini | $0.15 | $0.60 | â­â­â­â­â­ |
| GPT-4 Turbo | $10.00 | $10.00 | â­â­â­ |
| GPT-4 | $30.00 | $60.00 | â­â­ |

### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

```python
def select_model(task_type: str, budget: str = "medium") -> str:
    """ì‘ì—… ìœ í˜•ê³¼ ì˜ˆì‚°ì— ë”°ë¼ ìµœì  ëª¨ë¸ ì„ íƒ"""

    # ë³µì¡ë„ ë†’ì€ ì‘ì—…
    if task_type in ["complex_reasoning", "advanced_coding", "research"]:
        return "gpt-4.1" if budget == "high" else "gpt-4.1-mini"

    # ë©€í‹°ëª¨ë‹¬ ì‘ì—…
    elif task_type in ["image_analysis", "vision", "audio"]:
        return "gpt-4o" if budget == "high" else "gpt-4o-mini"

    # ì¼ë°˜ì ì¸ ì‘ì—…
    elif task_type in ["chat", "writing", "simple_coding"]:
        return "gpt-4.1-mini"

    # ëŒ€ëŸ‰ ì²˜ë¦¬
    elif task_type in ["classification", "extraction", "batch"]:
        return "gpt-4.1-nano" if budget == "low" else "gpt-4o-mini"

    # ê¸°ë³¸ê°’
    else:
        return "gpt-4.1-mini"

# ì‚¬ìš© ì˜ˆì‹œ
model = select_model("advanced_coding", "medium")
print(f"ì„ íƒëœ ëª¨ë¸: {model}")  # gpt-4.1-mini
```

### ìµœì‹  ê¸°ëŠ¥ í™œìš©

#### 1. JSON ëª¨ë“œ (Structured Outputs)

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {"role": "user", "content": "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: 'ì´ ì œí’ˆ ì •ë§ í›Œë¥­í•´ìš”!'"}
    ],
    response_format={"type": "json_object"}
)

print(response.choices[0].message.content)
# {"sentiment": "positive", "confidence": 0.95}
```

#### 2. Function Calling

```python
import json

# í•¨ìˆ˜ ì •ì˜
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "ë„ì‹œ ì´ë¦„, ì˜ˆ: ì„œìš¸"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"]
                    }
                },
                "required": ["location"]
            }
        }
    }
]

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?"}],
    tools=tools,
    tool_choice="auto"
)

# í•¨ìˆ˜ í˜¸ì¶œ í™•ì¸
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments)

    print(f"í•¨ìˆ˜: {function_name}")
    print(f"ì¸ì: {function_args}")
```

#### 3. ìŠ¤íŠ¸ë¦¬ë°

```python
stream = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "Pythonì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ

```python
import asyncio
from openai import AsyncOpenAI

async def process_batch(prompts: list[str]) -> list[str]:
    """ë¹„ë™ê¸°ë¡œ ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬"""
    client = AsyncOpenAI()

    async def process_single(prompt: str):
        response = await client.chat.completions.create(
            model="gpt-4.1-nano",  # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )
        return response.choices[0].message.content

    # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬
    tasks = [process_single(p) for p in prompts]
    results = await asyncio.gather(*tasks)

    return results

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    prompts = [
        "Pythonì´ë€?",
        "JavaScriptë€?",
        "TypeScriptë€?"
    ]

    results = await process_batch(prompts)
    for i, result in enumerate(results, 1):
        print(f"\n=== ê²°ê³¼ {i} ===")
        print(result)

# ì‹¤í–‰
# asyncio.run(main())
```

### 2. í”„ë¡¬í”„íŠ¸ ìºì‹± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¬ì‚¬ìš©)

```python
class CachedChatbot:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” íš¨ìœ¨ì ì¸ ì±—ë´‡"""

    def __init__(self, system_prompt: str):
        self.client = OpenAI()
        self.system_prompt = system_prompt
        self.conversation_history = [
            {"role": "system", "content": system_prompt}
        ]

    def chat(self, user_message: str) -> str:
        """ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ë°›ê¸°"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        # API í˜¸ì¶œ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìë™ìœ¼ë¡œ ìºì‹œë¨)
        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=self.conversation_history,
            max_tokens=1000
        )

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥
        assistant_message = response.choices[0].message.content
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_message
        })

        return assistant_message

# ì‚¬ìš© ì˜ˆì‹œ
chatbot = CachedChatbot(
    "ë‹¹ì‹ ì€ Python í”„ë¡œê·¸ë˜ë° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
    "ì½”ë“œ ì˜ˆì œì™€ í•¨ê»˜ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)

print(chatbot.chat("ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë­”ê°€ìš”?"))
print(chatbot.chat("ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"))
```

### 3. í† í° ê´€ë¦¬ ë° ìµœì í™”

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def optimize_prompt(prompt: str, max_tokens: int = 4000) -> str:
    """í”„ë¡¬í”„íŠ¸ë¥¼ í† í° ì œí•œ ë‚´ë¡œ ìµœì í™”"""
    tokens = count_tokens(prompt)

    if tokens <= max_tokens:
        return prompt

    # í† í° ì´ˆê³¼ ì‹œ ì˜ë¼ë‚´ê¸°
    encoding = tiktoken.encoding_for_model("gpt-4")
    encoded = encoding.encode(prompt)
    truncated = encoded[:max_tokens]

    return encoding.decode(truncated)

# ì‚¬ìš© ì˜ˆì‹œ
long_text = "..." * 1000  # ê¸´ í…ìŠ¤íŠ¸
optimized = optimize_prompt(long_text, max_tokens=2000)

print(f"ì›ë³¸ í† í°: {count_tokens(long_text)}")
print(f"ìµœì í™” í›„: {count_tokens(optimized)}")
```

### 4. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹„ìš© ì ˆê°

```python
from typing import List, Dict
import time

class BatchProcessor:
    """ëŒ€ëŸ‰ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬"""

    def __init__(self, model: str = "gpt-4.1-nano"):
        self.client = AsyncOpenAI()
        self.model = model
        self.results = []

    async def process_items(
        self,
        items: List[str],
        prompt_template: str,
        batch_size: int = 10
    ) -> List[Dict]:
        """ì•„ì´í…œ ë°°ì¹˜ ì²˜ë¦¬"""

        all_results = []

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]

            # ê° ë°°ì¹˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
            tasks = []
            for item in batch:
                prompt = prompt_template.format(item=item)
                task = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=200
                )
                tasks.append(task)

            # ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°
            responses = await asyncio.gather(*tasks)

            # ê²°ê³¼ ì €ì¥
            for item, response in zip(batch, responses):
                all_results.append({
                    "input": item,
                    "output": response.choices[0].message.content,
                    "tokens": response.usage.total_tokens
                })

            print(f"ì²˜ë¦¬ ì™„ë£Œ: {i+len(batch)}/{len(items)}")

            # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€
            if i + batch_size < len(items):
                await asyncio.sleep(1)

        return all_results

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    processor = BatchProcessor(model="gpt-4.1-nano")

    reviews = [
        "ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!",
        "ë°°ì†¡ì´ ë„ˆë¬´ ëŠë ¸ì–´ìš”.",
        "ê°€ì„±ë¹„ ìµœê³ ì…ë‹ˆë‹¤.",
        # ... ìˆ˜ë°± ê°œì˜ ë¦¬ë·°
    ]

    results = await processor.process_items(
        reviews,
        prompt_template="ë‹¤ìŒ ë¦¬ë·°ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš” (ê¸ì •/ë¶€ì •/ì¤‘ë¦½): '{item}'",
        batch_size=5
    )

    # ê²°ê³¼ í†µê³„
    total_tokens = sum(r["tokens"] for r in results)
    print(f"\nì´ í† í° ì‚¬ìš©: {total_tokens}")
    print(f"í‰ê·  í† í°/ê±´: {total_tokens/len(results):.0f}")

# asyncio.run(main())
```

---

## í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### 1. Zero-shot vs Few-shot

#### Zero-shot (ì˜ˆì‹œ ì—†ì´)

```python
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {"role": "user", "content": "ë‹¤ìŒ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”: 'ì •ë§ ì‹¤ë§ìŠ¤ëŸ¬ì› ì–´ìš”'"}
    ]
)
```

#### Few-shot (ì˜ˆì‹œ ì œê³µ)

```python
prompt = """
ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”:

ì˜ˆì‹œ 1:
ë¬¸ì¥: "ë„ˆë¬´ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤"
ê°ì •: ê¸ì •

ì˜ˆì‹œ 2:
ë¬¸ì¥: "ìµœì•…ì´ì—ˆì–´ìš”"
ê°ì •: ë¶€ì •

ì´ì œ ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ì„¸ìš”:
ë¬¸ì¥: "ê·¸ì € ê·¸ë¬ì–´ìš”"
ê°ì •:
"""

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": prompt}]
)
```

### 2. Chain of Thought (ì‚¬ê³ ì˜ ì—°ì‡„)

```python
cot_prompt = """
ë‹¤ìŒ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”:

ë¬¸ì œ: í•œ ìƒì ì—ì„œ ì‚¬ê³¼ 12ê°œë¥¼ ìƒ€ëŠ”ë°, 3ê°œê°€ ìƒí–ˆìŠµë‹ˆë‹¤.
ìƒí•˜ì§€ ì•Šì€ ì‚¬ê³¼ë¥¼ 4ëª…ì´ ë˜‘ê°™ì´ ë‚˜ëˆ  ê°€ì§„ë‹¤ë©´ í•œ ëª…ë‹¹ ëª‡ ê°œì”© ê°€ì§€ê²Œ ë˜ë‚˜ìš”?

ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë´…ì‹œë‹¤:
1. ë¨¼ì €...
"""

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": cot_prompt}]
)
```

### 3. ì—­í•  ë¶€ì—¬ (Role Prompting)

```python
system_prompts = {
    "ì „ë¬¸ê°€": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤.",
    "êµì‚¬": "ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…í•˜ëŠ” í›Œë¥­í•œ êµì‚¬ì…ë‹ˆë‹¤.",
    "ì°½ì˜ì ": "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ë…íŠ¹í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” í¬ë¦¬ì—ì´í‹°ë¸Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
}

def ask_with_role(question: str, role: str = "ì „ë¬¸ê°€") -> str:
    """íŠ¹ì • ì—­í• ì„ ë¶€ì—¬í•˜ì—¬ ì§ˆë¬¸"""
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompts[role]},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# ì‚¬ìš© ì˜ˆì‹œ
answer = ask_with_role("ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì´ ë­”ê°€ìš”?", role="êµì‚¬")
print(answer)
```

### 4. êµ¬ì¡°í™”ëœ ì¶œë ¥

```python
structured_prompt = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

í…ìŠ¤íŠ¸: "ì•„ì´í° 15 í”„ë¡œë¥¼ ìƒ€ëŠ”ë° ì¹´ë©”ë¼ ì„±ëŠ¥ì´ ì •ë§ í›Œë¥­í•´ìš”. ê°€ê²©ì´ ì¢€ ë¹„ì‹¸ê¸´ í•˜ì§€ë§Œ ë§Œì¡±í•©ë‹ˆë‹¤."

JSON í˜•ì‹:
{
  "product": "ì œí’ˆëª…",
  "sentiment": "positive/negative/neutral",
  "pros": ["ì¥ì 1", "ì¥ì 2"],
  "cons": ["ë‹¨ì 1", "ë‹¨ì 2"]
}
"""

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": structured_prompt}],
    response_format={"type": "json_object"}
)

import json
result = json.loads(response.choices[0].message.content)
print(json.dumps(result, ensure_ascii=False, indent=2))
```

---

## ë¹„ìš© ìµœì í™”

### 1. ëª¨ë¸ ì„ íƒ ì „ëµ

```python
class CostOptimizedAgent:
    """ë¹„ìš© ìµœì í™”ëœ AI ì—ì´ì „íŠ¸"""

    # 2025ë…„ ê°€ê²© (1M í† í° ê¸°ì¤€)
    PRICING = {
        "gpt-4.1-nano": {"input": 0.10, "output": 0.40},  # ì¶”ì •
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        "gpt-4.1-mini": {"input": 5.00, "output": 15.00},  # ì¶”ì •
        "gpt-4-turbo": {"input": 10.00, "output": 10.00},
        "gpt-4": {"input": 30.00, "output": 60.00}
    }

    def __init__(self):
        self.client = OpenAI()
        self.total_cost = 0.0
        self.usage_log = []

    def classify_task_complexity(self, task: str) -> str:
        """ì‘ì—… ë³µì¡ë„ ìë™ ë¶„ë¥˜"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        simple_keywords = ["ë¶„ë¥˜", "ê°ì •", "í‚¤ì›Œë“œ", "ìš”ì•½"]
        complex_keywords = ["ë¶„ì„", "ì¶”ë¡ ", "ìƒì„±", "ì½”ë“œ"]

        task_lower = task.lower()

        if any(kw in task_lower for kw in simple_keywords):
            return "simple"
        elif any(kw in task_lower for kw in complex_keywords):
            return "complex"
        else:
            return "medium"

    def select_cost_effective_model(self, task: str) -> str:
        """ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„ íƒ"""
        complexity = self.classify_task_complexity(task)

        if complexity == "simple":
            return "gpt-4.1-nano"
        elif complexity == "medium":
            return "gpt-4o-mini"
        else:
            return "gpt-4.1-mini"

    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        pricing = self.PRICING.get(model, self.PRICING["gpt-4o-mini"])

        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]

        return input_cost + output_cost

    def run(self, task: str, user_input: str) -> dict:
        """ì‘ì—… ì‹¤í–‰ ë° ë¹„ìš© ì¶”ì """
        # ìµœì  ëª¨ë¸ ì„ íƒ
        model = self.select_cost_effective_model(task)

        # API í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": f"ì‘ì—…: {task}"},
                {"role": "user", "content": user_input}
            ]
        )

        # ë¹„ìš© ê³„ì‚°
        usage = response.usage
        cost = self.calculate_cost(model, usage.prompt_tokens, usage.completion_tokens)
        self.total_cost += cost

        # ë¡œê·¸ ê¸°ë¡
        log_entry = {
            "task": task,
            "model": model,
            "input_tokens": usage.prompt_tokens,
            "output_tokens": usage.completion_tokens,
            "cost": cost
        }
        self.usage_log.append(log_entry)

        return {
            "response": response.choices[0].message.content,
            "model_used": model,
            "cost": f"${cost:.6f}",
            "total_cost": f"${self.total_cost:.6f}"
        }

# ì‚¬ìš© ì˜ˆì‹œ
agent = CostOptimizedAgent()

# ê°„ë‹¨í•œ ì‘ì—… (nano ì‚¬ìš©)
result1 = agent.run("ê°ì • ë¶„ë¥˜", "ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!")
print(f"ì‚¬ìš© ëª¨ë¸: {result1['model_used']}, ë¹„ìš©: {result1['cost']}")

# ë³µì¡í•œ ì‘ì—… (mini ì‚¬ìš©)
result2 = agent.run("ì½”ë“œ ìƒì„±", "ì´ì§„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì„¸ìš”")
print(f"ì‚¬ìš© ëª¨ë¸: {result2['model_used']}, ë¹„ìš©: {result2['cost']}")

print(f"\nì´ ë¹„ìš©: {result2['total_cost']}")
```

### 2. í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¡œ í† í° ì ˆì•½

```python
# âŒ ë¹„íš¨ìœ¨ì 
verbose_prompt = """
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë‹¹ì‹ ì—ê²Œ ì§ˆë¬¸ì´ í•˜ë‚˜ ìˆìŠµë‹ˆë‹¤.
ì œê°€ ê¶ê¸ˆí•œ ê²ƒì€ ë°”ë¡œ... ìŒ... ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œìš”?
ê·¸ëŸ¬ë‹ˆê¹Œ ì œ ë§ì€, Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.
ê°€ëŠ¥í•˜ë©´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì‹œë©´ ì •ë§ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!
ì—¬ëŸ¬ ë°©ë²•ì´ ìˆë‹¤ë©´ ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”.
"""

# âœ… íš¨ìœ¨ì 
concise_prompt = "Python ë¦¬ìŠ¤íŠ¸ ì •ë ¬ ë°©ë²•ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

# í† í° ë¹„êµ
print(f"ë¹„íš¨ìœ¨ì : {count_tokens(verbose_prompt)} í† í°")
print(f"íš¨ìœ¨ì : {count_tokens(concise_prompt)} í† í°")
# ë¹„íš¨ìœ¨ì : 87 í† í°
# íš¨ìœ¨ì : 12 í† í° (85% ì ˆê°!)
```

---

## ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: ê³ ì„±ëŠ¥ ì±—ë´‡

```python
from openai import OpenAI
from typing import List, Dict

class HighPerformanceChatbot:
    """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ê³ ì„±ëŠ¥ ì±—ë´‡"""

    def __init__(self, model: str = "gpt-4.1-mini"):
        self.client = OpenAI()
        self.model = model
        self.conversation: List[Dict] = []
        self.max_history = 10  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€

    def add_message(self, role: str, content: str):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation.append({"role": role, "content": content})

        # íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        if len(self.conversation) > self.max_history:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ëŒ€í™”ë§Œ ì œê±°
            system_msgs = [m for m in self.conversation if m["role"] == "system"]
            recent_msgs = [m for m in self.conversation if m["role"] != "system"][-self.max_history:]
            self.conversation = system_msgs + recent_msgs

    def chat(self, user_input: str) -> str:
        """ëŒ€í™” ì§„í–‰"""
        self.add_message("user", user_input)

        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.conversation
        )

        assistant_reply = response.choices[0].message.content
        self.add_message("assistant", assistant_reply)

        return assistant_reply

    def stream_chat(self, user_input: str):
        """ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™” (ì‹¤ì‹œê°„ ì‘ë‹µ)"""
        self.add_message("user", user_input)

        stream = self.client.chat.completions.create(
            model=self.model,
            messages=self.conversation,
            stream=True
        )

        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                yield content

        self.add_message("assistant", full_response)

# ì‚¬ìš© ì˜ˆì‹œëŠ” examples/chatbot.py ì°¸ì¡°
```

### ì˜ˆì œ 2: ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹œìŠ¤í…œ

```python
import asyncio
from openai import AsyncOpenAI
from typing import List, Dict
import time

class BatchTextClassifier:
    """ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë¥˜"""

    def __init__(self, model: str = "gpt-4.1-nano"):
        self.client = AsyncOpenAI()
        self.model = model

    async def classify_single(self, text: str, categories: List[str]) -> Dict:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ì¹´í…Œê³ ë¦¬: {', '.join(categories)}

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{"category": "ì„ íƒëœ ì¹´í…Œê³ ë¦¬", "confidence": 0.0-1.0}}
"""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )

        import json
        return json.loads(response.choices[0].message.content)

    async def classify_batch(
        self,
        texts: List[str],
        categories: List[str],
        batch_size: int = 10
    ) -> List[Dict]:
        """ë°°ì¹˜ ë¶„ë¥˜ (ë™ì‹œì„± ì œì–´)"""
        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë™ì‹œ ì²˜ë¦¬
            tasks = [self.classify_single(text, categories) for text in batch]
            batch_results = await asyncio.gather(*tasks)

            results.extend(batch_results)
            print(f"ì²˜ë¦¬: {len(results)}/{len(texts)}")

            # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€
            await asyncio.sleep(0.5)

        return results

# ì‚¬ìš© ì˜ˆì‹œëŠ” examples/batch_classifier.py ì°¸ì¡°
```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (ì‹¤ì¸¡)

| ëª¨ë¸ | ì§§ì€ ì‘ë‹µ (100í† í°) | ê¸´ ì‘ë‹µ (1000í† í°) | ì½”ë“œ ìƒì„± | ë¹„ìš© (1000ìš”ì²­) |
|-----|-------------------|------------------|----------|---------------|
| GPT-4.1 nano | 0.8ì´ˆ | 2.1ì´ˆ | â­â­â­ | $0.50 |
| GPT-4o mini | 0.9ì´ˆ | 2.3ì´ˆ | â­â­â­â­ | $0.75 |
| GPT-4.1 mini | 1.2ì´ˆ | 3.1ì´ˆ | â­â­â­â­â­ | $20 |
| GPT-4 Turbo | 1.5ì´ˆ | 3.8ì´ˆ | â­â­â­â­â­ | $100 |

---

## ë³´ì•ˆ ë° ëª¨ë²” ì‚¬ë¡€

### 1. API í‚¤ ê´€ë¦¬

```python
# âœ… ê¶Œì¥: í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
import os
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âŒ ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ: ì½”ë“œì— í•˜ë“œì½”ë”©
# client = OpenAI(api_key="sk-proj-...")
```

### 2. ì—ëŸ¬ ì²˜ë¦¬

```python
from openai import OpenAI, OpenAIError
import time

def robust_api_call(prompt: str, max_retries: int = 3):
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì•ˆì „í•œ API í˜¸ì¶œ"""
    client = OpenAI()

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content

        except OpenAIError as e:
            print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")

            if attempt < max_retries - 1:
                # ì§€ìˆ˜ ë°±ì˜¤í”„
                wait_time = 2 ** attempt
                print(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
            else:
                raise

# ì‚¬ìš©
try:
    result = robust_api_call("ì•ˆë…•í•˜ì„¸ìš”")
    print(result)
except OpenAIError as e:
    print(f"ìµœì¢… ì‹¤íŒ¨: {e}")
```

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [OpenAI ê³µì‹ ë¬¸ì„œ](https://platform.openai.com/docs)
- [OpenAI Cookbook](https://cookbook.openai.com/)
- [OpenAI API ë ˆí¼ëŸ°ìŠ¤](https://platform.openai.com/docs/api-reference)

---

**ì‘ì„±ì¼**: 2025-01-15
**ë²„ì „**: 1.0 (2025 ìµœì‹  ëª¨ë¸ ë°˜ì˜)
**ë¼ì´ì„ ìŠ¤**: MIT
